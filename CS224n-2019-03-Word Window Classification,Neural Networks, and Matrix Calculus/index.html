<!doctype html><html lang="zh" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="description" content="Looper's homepage"><link rel="canonical" href="https://looperxx.github.io/CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/"><meta name="author" content="Looper - Xiao Xu"><meta name="lang:clipboard.copy" content="复制"><meta name="lang:clipboard.copied" content="已复制"><meta name="lang:search.language" content="jp"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="没有找到符合条件的结果"><meta name="lang:search.result.one" content="找到 1 个符合条件的结果"><meta name="lang:search.result.other" content="# 个符合条件的结果"><meta name="lang:search.tokenizer" content="[\uff0c\u3002]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>03 Word Window Classification,Neural Networks, and Matrix Calculus - Science is interesting.</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#cs224n-2019" tabindex="1" class="md-skip">跳转至 </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-header-nav__button md-logo"><i class="md-icon"></i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Science is interesting.</span><span class="md-header-nav__topic">03 Word Window Classification,Neural Networks, and Matrix Calculus</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">键入以开始搜索</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div></div></div></nav></header><div class="md-container"><nav class="md-tabs md-tabs--active" data-md-component="tabs"><div class="md-tabs__inner md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a href=".." title="Home" class="md-tabs__link">Home</a></li><li class="md-tabs__item"><a href="../Linux/" title="Math & CS & Coding" class="md-tabs__link">Math & CS & Coding</a></li><li class="md-tabs__item"><a href="../Attention/" title="ML & DL" class="md-tabs__link">ML & DL</a></li><li class="md-tabs__item"><a href="../自然语言处理简介/" title="NLP" class="md-tabs__link">NLP</a></li><li class="md-tabs__item"><a href="../面经/" title="Interview experience" class="md-tabs__link">Interview experience</a></li><li class="md-tabs__item"><a href="../MkDocs_demo/" title="For MkDocs" class="md-tabs__link">For MkDocs</a></li></ul></div></nav><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-nav__button md-logo"><i class="md-icon"></i></a>Science is interesting.</label><div class="md-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2"><label class="md-nav__link" for="nav-2">Math & CS & Coding</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Math & CS & Coding</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Linux/" title="Linux" class="md-nav__link">Linux</a></li><li class="md-nav__item"><a href="../Coding Knowledge/" title="重点内容" class="md-nav__link">重点内容</a></li><li class="md-nav__item"><a href="../历年机试/" title="历年机试" class="md-nav__link">历年机试</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">ML & DL</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">ML & DL</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Attention/" title="Attention" class="md-nav__link">Attention</a></li><li class="md-nav__item"><a href="../Normalization/" title="Normalization" class="md-nav__link">Normalization</a></li><li class="md-nav__item"><a href="../Concepts/" title="Concepts" class="md-nav__link">Concepts</a></li><li class="md-nav__item"><a href="../花书经验法则/" title="花书经验法则" class="md-nav__link">花书经验法则</a></li><li class="md-nav__item"><a href="../经典网络/" title="经典网络" class="md-nav__link">经典网络</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked><label class="md-nav__link" for="nav-4">NLP</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">NLP</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1"><label class="md-nav__link" for="nav-4-1">简介</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-1">简介</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../自然语言处理简介/" title="自然语言处理简介" class="md-nav__link">自然语言处理简介</a></li><li class="md-nav__item"><a href="../NLP的巨人肩膀/" title="NLP的巨人肩膀" class="md-nav__link">NLP的巨人肩膀</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2"><label class="md-nav__link" for="nav-4-2">书籍笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-2">书籍笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../NLP Concepts/" title="NLP Concepts" class="md-nav__link">NLP Concepts</a></li><li class="md-nav__item"><a href="../Neural Reading Comprehension and beyond/" title="Machine Reading Comprehension" class="md-nav__link">Machine Reading Comprehension</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3" checked><label class="md-nav__link" for="nav-4-3">课程笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-3">课程笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-nav__link">CS224n-2019简介</a></li><li class="md-nav__item"><a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">CS224n-2019作业笔记</a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3" checked><label class="md-nav__link" for="nav-4-3-3">CS224n-2019学习笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="3"><label class="md-nav__title" for="nav-4-3-3">CS224n-2019学习笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">01 Introduction and Word Vectors</a></li><li class="md-nav__item"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">02 Word Vectors 2 and Word Senses</a></li><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">03 Word Window Classification,Neural Networks, and Matrix Calculus</label><a href="./" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link md-nav__link--active">03 Word Window Classification,Neural Networks, and Matrix Calculus</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" title="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#classification-setup-and-notation" title="Classification setup and notation" class="md-nav__link">Classification setup and notation</a></li><li class="md-nav__item"><a href="#neural-network-classifiers" title="Neural Network Classifiers" class="md-nav__link">Neural Network Classifiers</a></li><li class="md-nav__item"><a href="#named-entity-recognition-ner" title="Named Entity Recognition (NER)" class="md-nav__link">Named Entity Recognition (NER)</a></li><li class="md-nav__item"><a href="#binary-word-window-classification" title="Binary word window classification" class="md-nav__link">Binary word window classification</a></li><li class="md-nav__item"><a href="#gradients" title="Gradients" class="md-nav__link">Gradients</a></li></ul></nav></li><li class="md-nav__item"><a href="#notes-03-neural-networks-backpropagation" title="Notes 03 Neural Networks, Backpropagation" class="md-nav__link">Notes 03 Neural Networks, Backpropagation</a></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5"><label class="md-nav__link" for="nav-5">Interview experience</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-5">Interview experience</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../面经/" title="我的面经" class="md-nav__link">我的面经</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-2" type="checkbox" id="nav-5-2"><label class="md-nav__link" for="nav-5-2">实训笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-5-2">实训笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Protocol Buffers/" title="Protobuf" class="md-nav__link">Protobuf</a></li><li class="md-nav__item"><a href="../FDBus/" title="FDBus" class="md-nav__link">FDBus</a></li><li class="md-nav__item"><a href="../FDBus API/" title="FDBus API" class="md-nav__link">FDBus API</a></li><li class="md-nav__item"><a href="../FDBus内部结构/" title="FDBus内部结构" class="md-nav__link">FDBus内部结构</a></li><li class="md-nav__item"><a href="../Cross compiler/" title="Cross compiler" class="md-nav__link">Cross compiler</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6"><label class="md-nav__link" for="nav-6">For MkDocs</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-6">For MkDocs</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../MkDocs_demo/" title="Demo" class="md-nav__link">Demo</a></li><li class="md-nav__item"><a href="../Material Theme Tutorial/" title="Material Theme Tutorial" class="md-nav__link">Material Theme Tutorial</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" title="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#classification-setup-and-notation" title="Classification setup and notation" class="md-nav__link">Classification setup and notation</a></li><li class="md-nav__item"><a href="#neural-network-classifiers" title="Neural Network Classifiers" class="md-nav__link">Neural Network Classifiers</a></li><li class="md-nav__item"><a href="#named-entity-recognition-ner" title="Named Entity Recognition (NER)" class="md-nav__link">Named Entity Recognition (NER)</a></li><li class="md-nav__item"><a href="#binary-word-window-classification" title="Binary word window classification" class="md-nav__link">Binary word window classification</a></li><li class="md-nav__item"><a href="#gradients" title="Gradients" class="md-nav__link">Gradients</a></li></ul></nav></li><li class="md-nav__item"><a href="#notes-03-neural-networks-backpropagation" title="Notes 03 Neural Networks, Backpropagation" class="md-nav__link">Notes 03 Neural Networks, Backpropagation</a></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/looperxx/looperxx.github.io/edit/master/docs/CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a><h1 id="cs224n-2019">CS224n-2019 学习笔记<a class="headerlink" href="#cs224n-2019" title="Permanent link">&para;</a></h1>
<ul>
<li>
<p>结合每课时的课件、笔记与推荐读物等整理而成</p>
</li>
<li>
<p>作业部分将单独整理</p>
</li>
</ul>
<h2 id="lecture-03-word-window-classificationneural-networks-and-matrix-calculus">Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus<a class="headerlink" href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" title="Permanent link">&para;</a></h2>
<details class="abstract"><summary>Lecture Plan</summary><ul>
<li>Classification review/introduction</li>
<li>Neural networks introduction</li>
<li>Named Entity Recognition</li>
<li>Binary true vs. corrupted word window classification</li>
<li>Matrix calculus introduction</li>
</ul>
</details>
<div class="admonition info">
<p class="admonition-title">提示</p>
<p>这对一些人而言将是困难的一周，课后需要阅读提供的资料。</p>
</div>
<h3 id="classification-setup-and-notation">Classification setup and notation<a class="headerlink" href="#classification-setup-and-notation" title="Permanent link">&para;</a></h3>
<p>通常我们有由样本组成的训练数据集</p>
<div>
<div class="MathJax_Preview">
\left\{x_{i}, y_{i}\right\}_{i=1}^{N}
</div>
<script type="math/tex; mode=display">
\left\{x_{i}, y_{i}\right\}_{i=1}^{N}
</script>
</div>
<p><span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 是输入，例如单词（索引或是向量），句子，文档等等，维度为 <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span></p>
<p><span><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> 是我们尝试预测的标签（ <span><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> 个类别中的一个），例如：</p>
<ul>
<li>类别：感情，命名实体，购买/售出的决定</li>
<li>其他单词</li>
<li>之后：多词序列的</li>
</ul>
<p><strong>Classification intuition</strong></p>
<p><img alt="1560343189656" src="../imgs/1560343189656.png" /></p>
<p>训练数据： <span><span class="MathJax_Preview">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</span><script type="math/tex">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</script></span></p>
<p>简单的说明情况</p>
<ul>
<li>固定的二维单词向量分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习/统计学方法</strong>：假设 <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 是固定的，训练 softmax/logistic 回归的权重 <span><span class="MathJax_Preview">W \in \mathbb{R}^{C \times d}</span><script type="math/tex">W \in \mathbb{R}^{C \times d}</script></span> 来决定决定边界(超平面)</p>
<p><strong>方法</strong>：对每个 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> ，预测
$$
p(y | x)=\frac{\exp \left(W_{y} . x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}
$$
我们可以将预测函数分为两个步骤：</p>
<ol>
<li>
<p>将 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的 <span><span class="MathJax_Preview">y^{th}</span><script type="math/tex">y^{th}</script></span> 行和 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 中的对应行相乘得到分数
    $$
    W_{y} \cdot x=\sum_{i=1}^{d} W_{y i} x_{i}=f_{y}
    $$</p>
<p>计算所有的 <span><span class="MathJax_Preview">f_c, for \ c=1,\dots,C</span><script type="math/tex">f_c, for \ c=1,\dots,C</script></span></p>
</li>
<li>
<p>使用softmax函数获得归一化的概率</p>
</li>
</ol>
<div>
<div class="MathJax_Preview">
p(y | x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)
</div>
<script type="math/tex; mode=display">
p(y | x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)
</script>
</div>
<p><strong>Training with softmax and cross-entropy loss</strong></p>
<p>对于每个训练样本 <span><span class="MathJax_Preview">(x,y)</span><script type="math/tex">(x,y)</script></span> ，我们的目标是最大化正确类 <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 的概率，或者我们可以最小化该类的负对数概率
$$
-\log p(y | x)=-\log \left(\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}\right)
$$
<strong>Background: What is “cross entropy” loss/error?</strong></p>
<ul>
<li>交叉熵”的概念来源于信息论，衡量两个分布之间的差异</li>
<li>令真实概率分布为 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
<li>令我们计算的模型概率为 <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span></li>
<li>交叉熵为</li>
</ul>
<div>
<div class="MathJax_Preview">
H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)
</div>
<script type="math/tex; mode=display">
H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)
</script>
</div>
<ul>
<li>假设 groud truth (or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0：<span><span class="MathJax_Preview">p = [0,…,0,1,0,…0]</span><script type="math/tex">p = [0,…,0,1,0,…0]</script></span> </li>
<li>因为 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 是独热向量，所以唯一剩下的项是真实类的负对数概率</li>
</ul>
<p><strong>Classification over a full dataset</strong></p>
<p>在整个数据集 <span><span class="MathJax_Preview">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</span><script type="math/tex">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</script></span> 上的交叉熵损失函数，是所有样本的交叉熵的均值</p>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)
</script>
</div>
<p>我们不使用</p>
<div>
<div class="MathJax_Preview">
f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}
</div>
<script type="math/tex; mode=display">
f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}
</script>
</div>
<p>我们使用矩阵来表示 <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span></p>
<div>
<div class="MathJax_Preview">
f = Wx
</div>
<script type="math/tex; mode=display">
f = Wx
</script>
</div>
<p><strong>Traditional ML optimization</strong></p>
<ul>
<li>一般机器学习的参数 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 通常只由W的列组成</li>
</ul>
<div>
<div class="MathJax_Preview">
\theta=\left[\begin{array}{c}{W_{\cdot 1}} \\ {\vdots} \\ {W_{\cdot d}}\end{array}\right]=W( :) \in \mathbb{R}^{C d}
</div>
<script type="math/tex; mode=display">
\theta=\left[\begin{array}{c}{W_{\cdot 1}} \\ {\vdots} \\ {W_{\cdot d}}\end{array}\right]=W( :) \in \mathbb{R}^{C d}
</script>
</div>
<ul>
<li>因此，我们只通过以下方式更新决策边界</li>
</ul>
<div>
<div class="MathJax_Preview">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d}}}\end{array}\right] \in \mathbb{R}^{C d}
</div>
<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d}}}\end{array}\right] \in \mathbb{R}^{C d}
</script>
</div>
<h3 id="neural-network-classifiers">Neural Network Classifiers<a class="headerlink" href="#neural-network-classifiers" title="Permanent link">&para;</a></h3>
<p><img alt="1560345898614" src="../imgs/1560345898614.png" /></p>
<ul>
<li>单独使用Softmax(≈logistic回归)并不十分强大</li>
<li>Softmax只给出线性决策边界<ul>
<li>这可能是相当有限的，当问题很复杂时是无用的</li>
<li>纠正这些错误不是很酷吗?</li>
</ul>
</li>
</ul>
<p><strong>Neural Nets for the Win!</strong></p>
<p>神经网络可以学习更复杂的函数和非线性决策边界</p>
<p><img alt="1560346033994" src="../imgs/1560346033994.png" /></p>
<details class="tip"><summary>更高级的分类需要</summary><ul>
<li>词向量</li>
<li>更深层次的深层神经网络</li>
</ul>
</details>
<p><strong>Classification difference with word vectors</strong></p>
<p>一般在NLP深度学习中</p>
<ul>
<li>我们学习了矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 和词向量 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
<li>我们学习传统参数和表示</li>
<li>词向量是对独热向量的重新表示——在中间层向量空间中移动它们——以便使用(线性)softmax分类器通过 x = Le 层进行分类<ul>
<li>即将词向量理解为一层神经网络，输入单词的独热向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，<span><span class="MathJax_Preview">Vd</span><script type="math/tex">Vd</script></span> 是数量很大的参数</li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d a r d v a r k}}} \\ {\vdots} \\ {\nabla_{x_{z e b r a}}}\end{array}\right] \in \mathbb{R}^{C d + V d}
</div>
<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d a r d v a r k}}} \\ {\vdots} \\ {\nabla_{x_{z e b r a}}}\end{array}\right] \in \mathbb{R}^{C d + V d}
</script>
</div>
<p><strong>Neural computation</strong></p>
<p><img alt="1560346664232" src="../imgs/1560346664232.png" /></p>
<p><strong>An artificial neuron</strong></p>
<ul>
<li>神经网络有自己的术语包</li>
<li>但如果你了解 softmax 模型是如何工作的，那么你就可以很容易地理解神经元的操作</li>
</ul>
<p><img alt="1560346716435" src="../imgs/1560346716435.png" /></p>
<p><strong>A neuron can be a binary logistic regression unit</strong></p>
<p><span><span class="MathJax_Preview">f = nonlinear activation fct. (e.g. sigmoid), w = weights, b = bias, h = hidden, x = inputs</span><script type="math/tex">f = nonlinear activation fct. (e.g. sigmoid), w = weights, b = bias, h = hidden, x = inputs</script></span>
$$
\begin{array}{l}{h_{w, b}(x)=f\left(w^{\top} x+b\right)} \ {f(z)=\frac{1}{1+e^{-z}}}\end{array}
$$
<span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来</p>
<p><span><span class="MathJax_Preview">w,b</span><script type="math/tex">w,b</script></span> 是神经元的参数</p>
<p><strong>A neural network</strong>
<strong>= running several logistic regressions at the same time</strong></p>
<p><img alt="1560347357837" src="../imgs/1560347357837.png" /></p>
<p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p>
<p><img alt="1560347481494" src="../imgs/1560347481494.png" /></p>
<p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p>
<p><strong>Matrix notation for a layer</strong></p>
<p><img alt="1560347762809" src="../imgs/1560347762809.png" />
$$
\begin{array}{l}{a_{1}=f\left(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\right)} \ {a_{2}=f\left(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\right)}\ {z=W x+b} \ {a=f(z)} \ f\left(\left[z_{1}, z_{2}, z_{3}\right]\right)=\left[f\left(z_{1}\right), f\left(z_{2}\right), f\left(z_{3}\right)\right] \end{array}
$$</p>
<ul>
<li><span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 在运算时是 element-wise 逐元素的</li>
</ul>
<p><strong>Non-linearities (aka “f ”): Why they’re needed</strong></p>
<p>例如：函数近似，如回归或分类</p>
<ul>
<li>没有非线性，深度神经网络只能做线性变换</li>
<li>多个线性变换可以组成一个的线性变换 <span><span class="MathJax_Preview">W_1 W_2 x = Wx</span><script type="math/tex">W_1 W_2 x = Wx</script></span> <ul>
<li>因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li>
</ul>
</li>
<li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li>
</ul>
<h3 id="named-entity-recognition-ner">Named Entity Recognition (NER)<a class="headerlink" href="#named-entity-recognition-ner" title="Permanent link">&para;</a></h3>
<ul>
<li>任务：例如，查找和分类文本中的名称</li>
</ul>
<p><img alt="1560359392887" src="../imgs/1560359392887.png" /></p>
<ul>
<li>可能的用途<ul>
<li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li>
<li>对于问题回答，答案通常是命名实体</li>
<li>许多需要的信息实际上是命名实体之间的关联</li>
<li>同样的技术可以扩展到其他 slot-filling 槽填充 分类</li>
</ul>
</li>
<li>通常后面是命名实体链接/规范化到知识库</li>
</ul>
<p><strong>Named Entity Recognition on word sequences</strong></p>
<p><img alt="1560359650543" src="../imgs/1560359650543.png" /></p>
<p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p>
<p><strong>Why might NER be hard?</strong></p>
<ul>
<li>很难计算出实体的边界<ul>
<li><img alt="1560359674788" src="../imgs/1560359674788.png" /></li>
<li>第一个实体是 “First National Bank” 还是 “National Bank”</li>
</ul>
</li>
<li>很难知道某物是否是一个实体<ul>
<li>是一所名为“Future School” 的学校，还是这是一所未来的学校？</li>
</ul>
</li>
<li>很难知道未知/新奇实体的类别<ul>
<li><img alt="1560359774508" src="../imgs/1560359774508.png" /></li>
<li>“Zig Ziglar” ?  一个人</li>
</ul>
</li>
<li>实体类是模糊的，依赖于上下文<ul>
<li><img alt="1560359806734" src="../imgs/1560359806734.png" /></li>
<li>这里的“Charles Schwab”  是 PER
    不是 ORG</li>
</ul>
</li>
</ul>
<h3 id="binary-word-window-classification">Binary word window classification<a class="headerlink" href="#binary-word-window-classification" title="Permanent link">&para;</a></h3>
<p>为在上下文中的语言构建分类器</p>
<ul>
<li>一般来说，很少对单个单词进行分类</li>
<li>有趣的问题，如上下文歧义出现</li>
<li>例子：auto-antonyms<ul>
<li>"To sanction" can mean "to permit" or "to punish”</li>
<li>"To seed" can mean "to place seeds" or "to remove seeds"</li>
</ul>
</li>
<li>例子：解决模糊命名实体的链接<ul>
<li>Paris → Paris, France vs. Paris Hilton vs. Paris, Texas</li>
<li>Hathaway → Berkshire Hathaway vs. Anne Hathaway</li>
</ul>
</li>
</ul>
<p><strong>Window classification</strong></p>
<ul>
<li>思想：在**相邻词的上下文窗口**中对一个词进行分类</li>
<li>例如，上下文中一个单词的命名实体分类<ul>
<li>人、地点、组织、没有</li>
</ul>
</li>
<li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行**平均**，并对平均向量进行分类<ul>
<li>问题：<strong>这会丢失位置信息</strong></li>
</ul>
</li>
</ul>
<p><strong>Window classification: Softmax</strong></p>
<ul>
<li>训练softmax分类器对中心词进行分类，方法是在一个窗口内**将中心词周围的词向量串联起来**</li>
<li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li>
</ul>
<p><img alt="1560360448681" src="../imgs/1560360448681.png" /></p>
<ul>
<li>结果向量 <span><span class="MathJax_Preview">x_{window} = x \in R^{5d}</span><script type="math/tex">x_{window} = x \in R^{5d}</script></span>  是一个列向量</li>
</ul>
<p><strong>Simplest window classifier: Softmax</strong></p>
<p>对于 <span><span class="MathJax_Preview">x = x_{window}</span><script type="math/tex">x = x_{window}</script></span> ，我们可以使用与之前相同的softmax分类器</p>
<p><img alt="1560360599779" src="../imgs/1560360599779.png" /></p>
<ul>
<li>如何更新向量？</li>
<li>简而言之：就像上周那样，求导和优化</li>
</ul>
<p><strong>Binary classification with unnormalized scores</strong></p>
<ul>
<li>
<p>之前的例子：<span><span class="MathJax_Preview">X_{\text { window }}=[\begin{array}{ccc}{\mathrm{X}_{\text { museums }}} &amp; {\mathrm{X}_{\text { in }}} &amp; {\mathrm{X}_{\text { paris }} \quad \mathrm{X}_{\text { are }} \quad \mathrm{X}_{\text { amazing }} ]}\end{array}</span><script type="math/tex">X_{\text { window }}=[\begin{array}{ccc}{\mathrm{X}_{\text { museums }}} & {\mathrm{X}_{\text { in }}} & {\mathrm{X}_{\text { paris }} \quad \mathrm{X}_{\text { are }} \quad \mathrm{X}_{\text { amazing }} ]}\end{array}</script></span></p>
</li>
<li>
<p>假设我们要对中心词是否为一个地点，进行分类</p>
</li>
<li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分。</li>
<li>例如，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分</li>
</ul>
<p><strong>Binary classification for NER Location</strong></p>
<ul>
<li>
<p>例子：Not all museums in Paris are amazing</p>
</li>
<li>
<p>这里：一个真正的窗口，以Paris为中心的窗口和所有其他窗口都“损坏”了，因为它们的中心没有指定的实体位置。</p>
<ul>
<li>museums in Paris are amazing</li>
</ul>
</li>
<li>
<p>“损坏”窗口很容易找到，而且有很多：任何中心词没有在我们的语料库中明确标记为NER位置的窗口</p>
<ul>
<li>Not all museums in Paris</li>
</ul>
</li>
</ul>
<p><strong>Neural Network Feed-forward Computation</strong></p>
<p>使用神经激活 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 简单地给出一个非标准化的分数
$$
score(x)=U^{T} a \in \mathbb{R}
$$
我们用一个三层神经网络计算一个窗口的得分</p>
<ul>
<li><span><span class="MathJax_Preview">s = score("museums  \ in \ Paris \ are \ amazing”)</span><script type="math/tex">s = score("museums  \ in \ Paris \ are \ amazing”)</script></span></li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{array}{l}{s=U^{T} f(W x+b)} \\ {x \in \mathbb{R}^{20 \times 1}, W \in \mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}}\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}{s=U^{T} f(W x+b)} \\ {x \in \mathbb{R}^{20 \times 1}, W \in \mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}}\end{array}
</script>
</div>
<p><img alt="1560361207976" src="../imgs/1560361207976.png" /></p>
<p><strong>Main intuition for extra layer</strong></p>
<p>中间层学习输入词向量之间的**非线性交互**</p>
<p>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>
<p><strong>The max-margin loss</strong></p>
<p><img alt="1560361550807" src="../imgs/1560361550807.png" /></p>
<ul>
<li>关于训练目标的想法：让真实窗口的得分更高，而破坏窗口的得分更低(直到足够好为止)</li>
<li><span><span class="MathJax_Preview">s = score("museums  \ in \ Paris \ are \ amazing”)</span><script type="math/tex">s = score("museums  \ in \ Paris \ are \ amazing”)</script></span></li>
<li><span><span class="MathJax_Preview">s_c = score("Not \ all \ museums  \ in \ Paris)</span><script type="math/tex">s_c = score("Not \ all \ museums  \ in \ Paris)</script></span></li>
<li>最小化 <span><span class="MathJax_Preview">J=\max \left(0,1-s+s_{c}\right)</span><script type="math/tex">J=\max \left(0,1-s+s_{c}\right)</script></span></li>
<li>
<p>这是不可微的，但它是连续的→我们可以用SGD。</p>
</li>
<li>
<p>每个选项都是连续的</p>
</li>
<li>
<p>单窗口的目标函数为</p>
</li>
</ul>
<div>
<div class="MathJax_Preview">
J=\max \left(0,1-s+s_{c}\right)
</div>
<script type="math/tex; mode=display">
J=\max \left(0,1-s+s_{c}\right)
</script>
</div>
<ul>
<li>每个中心有NER位置的窗口的得分应该比中心没有位置的窗口高1分</li>
</ul>
<p><img alt="1560361673756" src="../imgs/1560361673756.png" /></p>
<ul>
<li>要获得完整的目标函数：为每个真窗口采样几个损坏的窗口。对所有培训窗口求和</li>
<li>
<p>类似于word2vec中的负抽样</p>
</li>
<li>
<p>使用SGD更新参数</p>
<ul>
<li><span><span class="MathJax_Preview">\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)</span><script type="math/tex">\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)</script></span></li>
<li><span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 是 步长或是学习率</li>
</ul>
</li>
<li>如何计算 <span><span class="MathJax_Preview">\nabla_{\theta} J(\theta)</span><script type="math/tex">\nabla_{\theta} J(\theta)</script></span> ？<ul>
<li>手工计算（本课）</li>
<li>算法：反向传播（下一课）</li>
</ul>
</li>
</ul>
<p><strong>Computing Gradients by Hand</strong></p>
<ul>
<li>回顾多元导数</li>
<li>矩阵微积分：完全矢量化的梯度<ul>
<li>比非矢量梯度快得多，也更有用</li>
<li>但做一个非矢量梯度可以是一个很好的实践；以上周的讲座为例</li>
<li><strong>notes</strong> 更详细地涵盖了这些材料</li>
</ul>
</li>
</ul>
<h3 id="gradients">Gradients<a class="headerlink" href="#gradients" title="Permanent link">&para;</a></h3>
<ul>
<li>给定一个函数，有1个输出和1个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
f(x) = x^3
</div>
<script type="math/tex; mode=display">
f(x) = x^3
</script>
</div>
<ul>
<li>斜率是它的导数</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{d f}{d x}=3 x^{2}
</div>
<script type="math/tex; mode=display">
\frac{d f}{d x}=3 x^{2}
</script>
</div>
<ul>
<li>给定一个函数，有1个输出和 n 个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
f(\boldsymbol{x})=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
</div>
<script type="math/tex; mode=display">
f(\boldsymbol{x})=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
</script>
</div>
<ul>
<li>梯度是关于每个输入的偏导数的向量</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial f}{\partial \boldsymbol{x}}=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial \boldsymbol{x}}=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right]
</script>
</div>
<p><strong>Jacobian Matrix: Generalization of the Gradient</strong></p>
<ul>
<li>给定一个函数，有 m 个输出和 n 个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
\boldsymbol{f}(\boldsymbol{x})=\left[f_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right), \ldots, f_{m}\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right]
</div>
<script type="math/tex; mode=display">
\boldsymbol{f}(\boldsymbol{x})=\left[f_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right), \ldots, f_{m}\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right]
</script>
</div>
<ul>
<li>其雅可比矩阵是一个<span><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span>的偏导矩阵</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{1}}{\partial x_{n}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial x_{n}}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}} & {\cdots} & {\frac{\partial f_{1}}{\partial x_{n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}} & {\cdots} & {\frac{\partial f_{m}}{\partial x_{n}}}\end{array}\right]
</script>
</div>
<div>
<div class="MathJax_Preview">
\left(\frac{\partial f}{\partial x}\right)_{i j}=\frac{\partial f_{i}}{\partial x_{j}}
</div>
<script type="math/tex; mode=display">
\left(\frac{\partial f}{\partial x}\right)_{i j}=\frac{\partial f_{i}}{\partial x_{j}}
</script>
</div>
<p><strong>Chain Rule</strong></p>
<p>对于单变量函数：乘以导数
$$
\begin{array}{l}{z=3 y} \ {y=x^{2}} \ {\frac{d z}{d x}=\frac{d z}{d y} \frac{d y}{d x}=(3)(2 x)=6 x}\end{array}
$$
对于一次处理多个变量：乘以雅可比矩阵
$$
\begin{array}{l}{\textbf{h}=f(\textbf{z})} \ {\textbf{z}=\textbf{W} \textbf{x}+\textbf{b}} \ {\frac{\partial \textbf{h}}{\partial \textbf{x}}=\frac{\partial \textbf{h}}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}}=\dots}\end{array}
$$
<strong>Example Jacobian: Elementwise activation Function</strong></p>
<p><span><span class="MathJax_Preview">h=f(z)</span><script type="math/tex">h=f(z)</script></span> , <span><span class="MathJax_Preview">\frac{\partial \textbf{h}}{\partial \textbf{z}} = ?, \textbf{h},\textbf{z} \in \mathbb{R}^{n}</span><script type="math/tex">\frac{\partial \textbf{h}}{\partial \textbf{z}} = ?, \textbf{h},\textbf{z} \in \mathbb{R}^{n}</script></span>  </p>
<p>由于使用的是 element-wise，所以 <span><span class="MathJax_Preview">h_{i}=f\left(z_{i}\right)</span><script type="math/tex">h_{i}=f\left(z_{i}\right)</script></span></p>
<p>函数有n个输出和n个输入 → n×n 的雅可比矩阵</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}\left(\frac{\partial h}{\partial z}\right)_{i j} &amp;=\frac{\partial h_{i}}{\partial z_{j}}=\frac{\partial}{\partial z_{j}} f\left(z_{i}\right), \text{definition of Jacobian} \\ &amp;=\left\{\begin{array}{ll}{f^{\prime}\left(z_{i}\right)} &amp; {\text { if } i=j} \\ {0} &amp; {\text { if otherwise }} , \text{regular 1-variable derivative} \end{array}\right.\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}\left(\frac{\partial h}{\partial z}\right)_{i j} &=\frac{\partial h_{i}}{\partial z_{j}}=\frac{\partial}{\partial z_{j}} f\left(z_{i}\right), \text{definition of Jacobian} \\ &=\left\{\begin{array}{ll}{f^{\prime}\left(z_{i}\right)} & {\text { if } i=j} \\ {0} & {\text { if otherwise }} , \text{regular 1-variable derivative} \end{array}\right.\end{aligned}
</script>
</div>
<div>
<div class="MathJax_Preview">
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}= \left(\begin{array}{ccc}{f^{\prime}\left(z_{1}\right)} &amp; { } &amp; {0} \\ {} &amp; {\ddots} &amp; { } \\ {0} &amp; { } &amp; {f^{\prime}\left(z_{n}\right)}\end{array}\right)=\operatorname{diag}\left(\boldsymbol{f}^{\prime}(\boldsymbol{z})\right)
</div>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}= \left(\begin{array}{ccc}{f^{\prime}\left(z_{1}\right)} & { } & {0} \\ {} & {\ddots} & { } \\ {0} & { } & {f^{\prime}\left(z_{n}\right)}\end{array}\right)=\operatorname{diag}\left(\boldsymbol{f}^{\prime}(\boldsymbol{z})\right)
</script>
</div>
<p><strong>Other Jacobians</strong>
$$
\begin{array}{l}{\frac{\partial}{\partial \boldsymbol{x}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{W}} \ {\frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{I} \text { (Identity matrix) }} \ \frac{\partial}{\partial \boldsymbol{u}}\left(\boldsymbol{u}^{T} \boldsymbol{h}\right)=\boldsymbol{h}^{\boldsymbol{T}} \end{array}
$$
这是正确的雅可比矩阵。稍后我们将讨论“形状约定”；用它则答案是 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 。</p>
<p><strong>Back to our Neural Net!</strong></p>
<p><img alt="1560363626037" src="../imgs/1560363626037.png" /></p>
<p>如何计算 <span><span class="MathJax_Preview">\frac{\partial s}{\partial b}</span><script type="math/tex">\frac{\partial s}{\partial b}</script></span> ？</p>
<p>实际上，我们关心的是损失的梯度，但是为了简单起见，我们将计算分数的梯度</p>
<p><strong>Break up equations into simple pieces</strong></p>
<p><img alt="1560363713598" src="../imgs/1560363713598.png" /></p>
<p><strong>Apply the chain rule</strong>
$$
\begin{array}{l}{s=\boldsymbol{u}^{T} \boldsymbol{h}} \ {\boldsymbol{h}=f(\boldsymbol{z})} \ {\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}} \ {\boldsymbol{x}} \text{ (input) }\end{array}
$$</p>
<div>
<div class="MathJax_Preview">
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
</div>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
</script>
</div>
<p><img alt="1560363929934" src="../imgs/1560363929934.png" /></p>
<p>如何计算 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \textbf{W}}</span><script type="math/tex">\frac{\partial s}{\partial \textbf{W}}</script></span> ？
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} \end{aligned}
$$
前两项是重复的，无须重复计算
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}=\boldsymbol{\delta} \ \boldsymbol{\delta} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\boldsymbol{u}^{T} \circ f^{\prime}(\boldsymbol{z}) \end{aligned}
$$
其中，<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 是局部误差符号</p>
<p><strong>Derivative with respect to Matrix: Output shape</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\boldsymbol{W} \in \mathbb{R}^{n \times m}</span><script type="math/tex">\boldsymbol{W} \in \mathbb{R}^{n \times m}</script></span> ，<span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}</script></span> 的形状是</li>
<li>1个输出，<span><span class="MathJax_Preview">n \times m</span><script type="math/tex">n \times m</script></span> 个输入：1 × nm 的雅可比矩阵？<ul>
<li>不方便更新参数 <span><span class="MathJax_Preview">\theta^{n e w}=\theta^{o l d}-\alpha \nabla_{\theta} J(\theta)</span><script type="math/tex">\theta^{n e w}=\theta^{o l d}-\alpha \nabla_{\theta} J(\theta)</script></span></li>
</ul>
</li>
<li>而是遵循惯例：导数的形状是参数的形状 （形状约定）<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}</script></span> 的形状是 <span><span class="MathJax_Preview">n \times m</span><script type="math/tex">n \times m</script></span> </li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\left[\begin{array}{ccc}{\frac{\partial s}{\partial W_{11}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{1 m}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial s}{\partial W_{n 1}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{n m}}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\left[\begin{array}{ccc}{\frac{\partial s}{\partial W_{11}}} & {\cdots} & {\frac{\partial s}{\partial W_{1 m}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial s}{\partial W_{n 1}}} & {\cdots} & {\frac{\partial s}{\partial W_{n m}}}\end{array}\right]
</script>
</div>
<p><strong>Derivative with respect to Matrix</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}</script></span><ul>
<li><span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 将出现在我们的答案中</li>
<li>另一项应该是 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> ，因为 <span><span class="MathJax_Preview">\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}</span><script type="math/tex">\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}</script></span> </li>
</ul>
</li>
<li>这表明 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}</script></span></li>
</ul>
<p><img alt="1560364755148" src="../imgs/1560364755148.png" /></p>
<p><strong>Why the Transposes?</strong>
$$
\begin{array}{l}{\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \quad \boldsymbol{x}^{T}} \ {[n \times m]} {[n \times 1]} {[1 \times m]}\end{array}
$$</p>
<ul>
<li>粗糙的回答是：这样就可以解决尺寸问题了<ul>
<li>检查工作的有用技巧</li>
</ul>
</li>
<li>课堂讲稿中有完整的解释<ul>
<li>每个输入到每个输出——你得到的是外部积</li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}=\left[\begin{array}{c}{\delta_{1}} \\ {\vdots} \\ {\delta_{n}}\end{array}\right]\left[x_{1}, \ldots, x_{m}\right]=\left[\begin{array}{ccc}{\delta_{1} x_{1}} &amp; {\dots} &amp; {\delta_{1} x_{m}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\delta_{n} x_{1}} &amp; {\dots} &amp; {\delta_{n} x_{m}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}=\left[\begin{array}{c}{\delta_{1}} \\ {\vdots} \\ {\delta_{n}}\end{array}\right]\left[x_{1}, \ldots, x_{m}\right]=\left[\begin{array}{ccc}{\delta_{1} x_{1}} & {\dots} & {\delta_{1} x_{m}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\delta_{n} x_{1}} & {\dots} & {\delta_{n} x_{m}}\end{array}\right]
</script>
</div>
<p><strong>What shape should derivatives be?</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{b}}=\boldsymbol{h}^{T} \circ f^{\prime}(\boldsymbol{z})</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{b}}=\boldsymbol{h}^{T} \circ f^{\prime}(\boldsymbol{z})</script></span> 是行向量<ul>
<li>但是习惯上说梯度应该是一个列向量因为 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 是一个列向量</li>
</ul>
</li>
<li>
<p>雅可比矩阵形式(这使得链式法则很容易)和形状约定(这使得SGD很容易实现)之间的分歧</p>
<ul>
<li>我们希望答案遵循形状约定</li>
<li>但是雅可比矩阵形式对于计算答案很有用</li>
</ul>
</li>
<li>
<p>两个选择</p>
<ul>
<li>尽量使用雅可比矩阵形式，最后按照约定进行整形<ul>
<li>我们刚刚做的。但最后转置 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{b}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{b}}</script></span> 使导数成为列向量，得到 <span><span class="MathJax_Preview">\delta ^ T</span><script type="math/tex">\delta ^ T</script></span></li>
</ul>
</li>
<li>始终遵循惯例<ul>
<li>查看维度，找出何时转置 和/或 重新排序项。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>反向传播</p>
<ul>
<li>算法高效地计算梯度</li>
<li>将我们刚刚手工完成的转换成算法</li>
<li>用于深度学习软件框架(TensorFlow, PyTorch, Chainer, etc.)</li>
</ul>
<h2 id="notes-03-neural-networks-backpropagation">Notes 03 Neural Networks, Backpropagation<a class="headerlink" href="#notes-03-neural-networks-backpropagation" title="Permanent link">&para;</a></h2>
<details class="abstract"><summary>Keyphrases</summary><p>Neural networks.Forward computation.Backward.propagation.Neuron Units.Max-margin Loss.Gradient checks.Xavier parameter initialization.Learning rates.Adagrad.</p>
</details>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p><h2 id="__comments">评论</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
      this.page.url = "https://looperxx.github.io/CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/";
      this.page.identifier =
        "CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//https-looperxx-github-io-my-wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();</script></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">后退</span>02 Word Vectors 2 and Word Senses</span></div></a><a href="../面经/" title="我的面经" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">前进</span>我的面经</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright"><div class="md-footer-copyright__highlight">Copyright &copy; 2019 - 2020 Looper Xiao Xu</div>powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../assets/fonts/font-awesome.css"> <a href="https://github.com/looperXX" class="md-footer-social__link fa fa-github"></a>  <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" class="md-footer-social__link fa fa-linkedin"></a> </div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="../js/baidu-tongji.js"></script></body></html>