<!doctype html><html lang="zh" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="description" content="Looper's homepage"><link rel="canonical" href="https://looperxx.github.io/CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models/"><meta name="author" content="Looper - Xiao Xu"><meta name="lang:clipboard.copy" content="复制"><meta name="lang:clipboard.copied" content="已复制"><meta name="lang:search.language" content="jp"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="没有找到符合条件的结果"><meta name="lang:search.result.one" content="找到 1 个符合条件的结果"><meta name="lang:search.result.other" content="# 个符合条件的结果"><meta name="lang:search.tokenizer" content="[\uff0c\u3002]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>06 The probability of a sentence Recurrent Neural Networks and Language Models - Science is interesting.</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#cs224n-2019" tabindex="1" class="md-skip">跳转至 </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-header-nav__button md-logo"><i class="md-icon"></i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Science is interesting.</span><span class="md-header-nav__topic">06 The probability of a sentence Recurrent Neural Networks and Language Models</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">键入以开始搜索</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div></div></div></nav></header><div class="md-container"><nav class="md-tabs md-tabs--active" data-md-component="tabs"><div class="md-tabs__inner md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a href=".." title="Home" class="md-tabs__link">Home</a></li><li class="md-tabs__item"><a href="../Linux/" title="Math & CS & Coding" class="md-tabs__link">Math & CS & Coding</a></li><li class="md-tabs__item"><a href="../Attention/" title="ML & DL" class="md-tabs__link">ML & DL</a></li><li class="md-tabs__item"><a href="../自然语言处理简介/" title="NLP" class="md-tabs__link">NLP</a></li><li class="md-tabs__item"><a href="../面经/" title="Interview experience" class="md-tabs__link">Interview experience</a></li><li class="md-tabs__item"><a href="../MkDocs_demo/" title="For MkDocs" class="md-tabs__link">For MkDocs</a></li></ul></div></nav><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-nav__button md-logo"><i class="md-icon"></i></a>Science is interesting.</label><div class="md-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2"><label class="md-nav__link" for="nav-2">Math & CS & Coding</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Math & CS & Coding</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Linux/" title="Linux" class="md-nav__link">Linux</a></li><li class="md-nav__item"><a href="../Coding Knowledge/" title="重点内容" class="md-nav__link">重点内容</a></li><li class="md-nav__item"><a href="../历年机试/" title="历年机试" class="md-nav__link">历年机试</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">ML & DL</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">ML & DL</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Attention/" title="Attention" class="md-nav__link">Attention</a></li><li class="md-nav__item"><a href="../Normalization/" title="Normalization" class="md-nav__link">Normalization</a></li><li class="md-nav__item"><a href="../Concepts/" title="Concepts" class="md-nav__link">Concepts</a></li><li class="md-nav__item"><a href="../花书经验法则/" title="花书经验法则" class="md-nav__link">花书经验法则</a></li><li class="md-nav__item"><a href="../经典网络/" title="经典网络" class="md-nav__link">经典网络</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked><label class="md-nav__link" for="nav-4">NLP</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">NLP</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1"><label class="md-nav__link" for="nav-4-1">简介</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-1">简介</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../自然语言处理简介/" title="自然语言处理简介" class="md-nav__link">自然语言处理简介</a></li><li class="md-nav__item"><a href="../NLP的巨人肩膀/" title="NLP的巨人肩膀" class="md-nav__link">NLP的巨人肩膀</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2"><label class="md-nav__link" for="nav-4-2">书籍笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-2">书籍笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../NLP Concepts/" title="NLP Concepts" class="md-nav__link">NLP Concepts</a></li><li class="md-nav__item"><a href="../Neural Reading Comprehension and beyond/" title="Machine Reading Comprehension" class="md-nav__link">Machine Reading Comprehension</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3" checked><label class="md-nav__link" for="nav-4-3">课程笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-3">课程笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-nav__link">CS224n-2019简介</a></li><li class="md-nav__item"><a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">CS224n-2019作业笔记</a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3" checked><label class="md-nav__link" for="nav-4-3-3">CS224n-2019学习笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="3"><label class="md-nav__title" for="nav-4-3-3">CS224n-2019学习笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">01 Introduction and Word Vectors</a></li><li class="md-nav__item"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">02 Word Vectors 2 and Word Senses</a></li><li class="md-nav__item"><a href="../CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">03 Word Window Classification,Neural Networks, and Matrix Calculus</a></li><li class="md-nav__item"><a href="../CS224n-2019-04-Backpropagation and Computation Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">04 Backpropagation and Computation Graphs</a></li><li class="md-nav__item"><a href="../CS224n-2019-05-Linguistic Structure Dependency Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">05 Linguistic Structure Dependency Parsing</a></li><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">06 The probability of a sentence Recurrent Neural Networks and Language Models</label><a href="./" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link md-nav__link--active">06 The probability of a sentence Recurrent Neural Networks and Language Models</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" title="Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models</a></li><li class="md-nav__item"><a href="#notes-05-language-models-rnn-gru-and-lstm" title="Notes 05 Language Models, RNN, GRU and LSTM" class="md-nav__link">Notes 05 Language Models, RNN, GRU and LSTM</a></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5"><label class="md-nav__link" for="nav-5">Interview experience</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-5">Interview experience</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../面经/" title="我的面经" class="md-nav__link">我的面经</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-2" type="checkbox" id="nav-5-2"><label class="md-nav__link" for="nav-5-2">实训笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-5-2">实训笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Protocol Buffers/" title="Protobuf" class="md-nav__link">Protobuf</a></li><li class="md-nav__item"><a href="../FDBus/" title="FDBus" class="md-nav__link">FDBus</a></li><li class="md-nav__item"><a href="../FDBus API/" title="FDBus API" class="md-nav__link">FDBus API</a></li><li class="md-nav__item"><a href="../FDBus内部结构/" title="FDBus内部结构" class="md-nav__link">FDBus内部结构</a></li><li class="md-nav__item"><a href="../Cross compiler/" title="Cross compiler" class="md-nav__link">Cross compiler</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6"><label class="md-nav__link" for="nav-6">For MkDocs</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-6">For MkDocs</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../MkDocs_demo/" title="Demo" class="md-nav__link">Demo</a></li><li class="md-nav__item"><a href="../Material Theme Tutorial/" title="Material Theme Tutorial" class="md-nav__link">Material Theme Tutorial</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" title="Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models</a></li><li class="md-nav__item"><a href="#notes-05-language-models-rnn-gru-and-lstm" title="Notes 05 Language Models, RNN, GRU and LSTM" class="md-nav__link">Notes 05 Language Models, RNN, GRU and LSTM</a></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/looperxx/looperxx.github.io/edit/master/docs/CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a><h1 id="cs224n-2019">CS224n-2019 学习笔记<a class="headerlink" href="#cs224n-2019" title="Permanent link">&para;</a></h1>
<ul>
<li>结合每课时的课件、笔记与推荐读物等整理而成</li>
<li>作业部分将单独整理</li>
</ul>
<h2 id="lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models">Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models<a class="headerlink" href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" title="Permanent link">&para;</a></h2>
<details class="abstract"><summary>Overview</summary><ul>
<li>介绍一个新的NLP任务<ul>
<li>Language Modeling (motivate RNNs)</li>
</ul>
</li>
<li>介绍一个新的神经网络家族<ul>
<li><strong>Recurrent Neural Networks (RNNs)</strong></li>
</ul>
</li>
</ul>
</details>
<p><strong>Language Modeling</strong></p>
<ul>
<li>语言建模的任务是预测下一个单词是什么。</li>
</ul>
<p><img alt="1560953937807" src="../imgs/1560953937807.png" /></p>
<ul>
<li>更正式的说法是：给定一个单词序列 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</script></span> ，计算下一个单词 <span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 的概率分布</li>
</ul>
<div>
<div class="MathJax_Preview">
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
</div>
<script type="math/tex; mode=display">
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
</script>
</div>
<ul>
<li>其中，<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 可以是词表中的任意单词 <span><span class="MathJax_Preview">V=\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{|V|}\right\}</span><script type="math/tex">V=\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{|V|}\right\}</script></span></li>
<li>这样做的系统称为 <strong>Language Model</strong> 语言模型</li>
<li>还可以将语言模型看作是一个将概率分配给一段文本的系统</li>
<li>例如，如果我们有一段文本 <span><span class="MathJax_Preview">x^{(1)},\dots,x^{(T)}</span><script type="math/tex">x^{(1)},\dots,x^{(T)}</script></span>  则这段文本的概率(根据语言模型)为</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right) &amp;=P\left(\boldsymbol{x}^{(1)}\right) \times P\left(\boldsymbol{x}^{(2)} | \boldsymbol{x}^{(1)}\right) \times \cdots \times P\left(\boldsymbol{x}^{(T)} | \boldsymbol{x}^{(T-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \\ &amp;=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right) &=P\left(\boldsymbol{x}^{(1)}\right) \times P\left(\boldsymbol{x}^{(2)} | \boldsymbol{x}^{(1)}\right) \times \cdots \times P\left(\boldsymbol{x}^{(T)} | \boldsymbol{x}^{(T-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \\ &=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \end{aligned}
</script>
</div>
<ul>
<li>语言模型提供的是 <span><span class="MathJax_Preview">\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right)</span><script type="math/tex">\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right)</script></span></li>
</ul>
<p><strong>n-gram Language Models</strong></p>
<div>
<div class="MathJax_Preview">
\text{the students opened their  ______}
</div>
<script type="math/tex; mode=display">
\text{the students opened their  ______}
</script>
</div>
<ul>
<li><strong><u>问题</u></strong> ：如何学习一个语言模型？</li>
<li><strong><u>回答</u></strong> (pre-DeepLearning)：学习一个 n-gram 语言模型</li>
<li><u>定义</u> ：n-gram 是 一个由n个连续单词组成的块<ul>
<li>unigrams: “the”, “students”, “opened”, ”their”</li>
<li>bigrams: “the students”, “students opened”, “opened their”</li>
<li>trigrams: “the students opened”, “students opened their” </li>
<li>4-grams: “the students opened their”</li>
</ul>
</li>
<li>
<p><strong>想法</strong> ：收集关于不同n-gram出现频率的统计数据，并使用这些数据预测下一个单词。</p>
</li>
<li>
<p>首先，我们做一个 <strong>简化假设</strong> ：<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 只依赖于前面的n-1个单词</p>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned}
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
&amp; =P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)
\\ &amp;=\frac{P\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{P\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
& =P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)
\\ &=\frac{P\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{P\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
\end{aligned}
</script>
</div>
<p>具体含义如下图所示</p>
<p><img alt="1560954979111" src="../imgs/1560954979111.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ：如何得到n-gram和(n-1)-gram的概率？</li>
<li><strong><u>回答</u></strong> ：通过在一些大型文本语料库中计算它们(统计近似)</li>
</ul>
<div>
<div class="MathJax_Preview">
\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
</div>
<script type="math/tex; mode=display">
\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
</script>
</div>
<p>假设我们正在学习一个 <strong>4-gram</strong> 的语言模型</p>
<p><img alt="1560955135094" src="../imgs/1560955135094.png" /></p>
<p>例如，假设在语料库中：</p>
<ul>
<li>“students opened their” 出现了1000次</li>
<li>“students opened their books” 出现了400次<ul>
<li><span><span class="MathJax_Preview">P(\text { books } | \text { students opened their })=0.4</span><script type="math/tex">P(\text { books } | \text { students opened their })=0.4</script></span></li>
</ul>
</li>
<li>“students opened their exams” 出现了100次<ul>
<li><span><span class="MathJax_Preview">P(\text { exams } | \text { students opened their })=0.1</span><script type="math/tex">P(\text { exams } | \text { students opened their })=0.1</script></span></li>
</ul>
</li>
<li>我们应该忽视上下文中的“proctor”吗？<ul>
<li>在本例中，上下文里出现了“proctor”，所以exams在这里的上下文中应该是比books概率更大的。</li>
</ul>
</li>
</ul>
<p><strong>Sparsity Problems with n-gram Language Models</strong></p>
<p><img alt="1560955544533" src="../imgs/1560955544533.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ：如果“students open their <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>” 从未出现在数据中，那么概率值为 0</li>
<li><strong><u>(Partial)解决方案</u></strong> ：为每个 <span><span class="MathJax_Preview">w\in V</span><script type="math/tex">w\in V</script></span> 添加极小数 <span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 。这叫做平滑。这使得词表中的每个单词都至少有很小的概率。</li>
<li><strong><u>问题</u></strong> ：如果“students open their” 从未出现在数据中，那么我们将无法计算任何单词 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的概率值</li>
<li><strong><u>(Partial)解决方案</u></strong> ：将条件改为“open their”。这叫做后退。</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>n 的增加使稀疏性问题变得更糟。一般情况下 n 不能大于5。</p>
</div>
<p><strong>Storage Problems with n-gram Language Models</strong></p>
<p><img alt="1560956032892" src="../imgs/1560956032892.png" /></p>
<p>增加 n 或增加语料库都会增加模型大小</p>
<p><strong>n-gram Language Models in practice</strong></p>
<ul>
<li>你可以在你的笔记本电脑上，在几秒钟内建立一个超过170万个单词库(Reuters)的简单的三元组语言模型</li>
<li>Reuters 是 商业和金融新闻的数据集</li>
</ul>
<p><img alt="1560956304459" src="../imgs/1560956304459.png" /></p>
<p><strong><u>稀疏性问题</u></strong> ：概率分布的粒度不大。“today the company” 和 “today the bank”都是<span><span class="MathJax_Preview">\frac{4}{26}</span><script type="math/tex">\frac{4}{26}</script></span> ，都只出现过四次</p>
<p><strong>Generating text with a n-gram Language Model</strong></p>
<ul>
<li>还可以使用语言模型来生成文本</li>
</ul>
<p><img alt="1560956554190" src="../imgs/1560956554190.png" /></p>
<p><img alt="1560956564197" src="../imgs/1560956564197.png" /></p>
<p><img alt="1560956573012" src="../imgs/1560956573012.png" /></p>
<p>使用trigram运行以上生成过程时，会得到如下文本</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>today the price of gold per ton , while production of shoe lasts and shoe industry , the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks , sept 30 end primary 76 cts a share .
</pre></div>
</td></tr></table>

<p><strong>令人惊讶的是其具有语法但是是不连贯的。如果我们想要很好地模拟语言，我们需要同时考虑三个以上的单词。但增加 n 使模型的稀疏性问题恶化，模型尺寸增大。</strong></p>
<p><strong>How to build a neural Language Model?</strong></p>
<ul>
<li>回忆一下语言模型任务<ul>
<li>输入：单词序列 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</script></span></li>
<li>输出：下一个单词的概率分布 <span><span class="MathJax_Preview">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)</span><script type="math/tex">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)</script></span></li>
</ul>
</li>
</ul>
<p>window-based neural model 在第三讲中被用于NER问题</p>
<p><img alt="1560956976214" src="../imgs/1560956976214.png" /></p>
<p><strong>A fixed-window neural Language Model</strong></p>
<p><img alt="1560957097608" src="../imgs/1560957097608.png" /></p>
<p>使用和NER问题中同样网络结构</p>
<p><img alt="1560957211434" src="../imgs/1560957211434.png" /></p>
<p>超越 n-gram 语言模型的 <strong>改进</strong></p>
<ul>
<li>没有稀疏性问题</li>
<li>不需要观察到所有的n-grams</li>
</ul>
<p>存在的问题</p>
<ul>
<li>固定窗口太小</li>
<li>扩大窗口就需要扩大权重矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span></li>
<li>窗口再大也不够用</li>
<li><span><span class="MathJax_Preview">x^{(1)}</span><script type="math/tex">x^{(1)}</script></span> 和 <span><span class="MathJax_Preview">x^{(2)}</span><script type="math/tex">x^{(2)}</script></span> 乘以完全不同的权重。输入的处理 <strong>不对称</strong>。</li>
</ul>
<p>我们需要一个神经结构，可以处理任何长度的输入</p>
<p><strong>Recurrent Neural Networks (RNN)</strong></p>
<p><u>核心想法</u>：重复使用 <strong>相同</strong> 的权重矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span></p>
<p><img alt="1560957560354" src="../imgs/1560957560354.png" /></p>
<p><img alt="1560958598963" src="../imgs/1560958598963.png" /></p>
<p>RNN的 <strong>优点</strong></p>
<ul>
<li>可以处理 <strong>任意长度</strong> 的输入</li>
<li>步骤 t 的计算(理论上)可以使用 <strong>许多步骤前</strong> 的信息</li>
<li><strong>模型大小不会</strong> 随着输入的增加而**增加**</li>
<li>在每个时间步上应用相同的权重，因此在处理输入时具有 <strong>对称性</strong></li>
</ul>
<p>RNN的 <strong>缺点</strong></p>
<ul>
<li>递归计算速度 <strong>慢</strong></li>
<li>在实践中，很难从 **许多步骤前**返回信息</li>
<li>后面的课程中会详细介绍</li>
</ul>
<p><strong>Training a RNN Language Model</strong></p>
<ul>
<li>获取一个较大的文本语料库，该语料库是一个单词序列</li>
<li>输入RNN-LM；计算每个步骤 t 的输出分布<ul>
<li>即预测到目前为止给定的每个单词的概率分布</li>
</ul>
</li>
<li>步骤 t 上的损失函数为预测概率分布 <span><span class="MathJax_Preview">\hat{\boldsymbol{y}}^{(t)}</span><script type="math/tex">\hat{\boldsymbol{y}}^{(t)}</script></span> 与真实下一个单词 <span><span class="MathJax_Preview">{\boldsymbol{y}}^{(t)}</span><script type="math/tex">{\boldsymbol{y}}^{(t)}</script></span> (<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 的独热向量)之间的交叉熵</li>
</ul>
<div>
<div class="MathJax_Preview">
J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)}, \hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V} \boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</div>
<script type="math/tex; mode=display">
J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)}, \hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V} \boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</script>
</div>
<ul>
<li>将其平均，得到整个培训集的 <strong>总体损失</strong></li>
</ul>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</script>
</div>
<p><img alt="1560959454417" src="../imgs/1560959454417.png" /></p>
<div>
<div class="MathJax_Preview">
J^{(1)}(\theta)+J^{(2)}(\theta)+J^{(3)}(\theta)+J^{(4)}(\theta)+\ldots=J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</div>
<script type="math/tex; mode=display">
J^{(1)}(\theta)+J^{(2)}(\theta)+J^{(3)}(\theta)+J^{(4)}(\theta)+\ldots=J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</script>
</div>
<ul>
<li>然而：计算 <strong>整个语料库</strong>  <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</script></span>的损失和梯度太昂贵了</li>
</ul>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</script>
</div>
<ul>
<li>在实践中，我们通常将 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</script></span> 看做一个 <strong>句子</strong> 或是 <strong>文档</strong></li>
<li><u>回忆</u> ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li>
<li>计算一个句子的损失<span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span>(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li>
</ul>
<p><strong>Backpropagation for RNNs</strong></p>
<p><img alt="1560960125900" src="../imgs/1560960125900.png" /></p>
<p><strong><u>问题</u></strong> ：关于 <strong>重复的</strong> 权重矩阵 <span><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 的偏导数 <span><span class="MathJax_Preview">J^{(t)}(\theta)</span><script type="math/tex">J^{(t)}(\theta)</script></span> </p>
<p><strong><u>回答</u></strong> ：重复权重的梯度是每次其出现时的梯度的总和</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}=\sum_{i=1}^{t}\left.\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}
</div>
<script type="math/tex; mode=display">
\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}=\sum_{i=1}^{t}\left.\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}
</script>
</div>
<p><strong>Multivariable Chain Rule</strong></p>
<p><img alt="1560960413574" src="../imgs/1560960413574.png" /></p>
<p>对于一个多变量函数 <span><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span> 和两个单变量函数 <span><span class="MathJax_Preview">x(t)</span><script type="math/tex">x(t)</script></span> 和 <span><span class="MathJax_Preview">y(t)</span><script type="math/tex">y(t)</script></span> ，其链式法则如下：</p>
<div>
<div class="MathJax_Preview">
\frac{d}{d t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{d x}{d t}+\frac{\partial f}{\partial y} \frac{d y}{d t}
</div>
<script type="math/tex; mode=display">
\frac{d}{d t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{d x}{d t}+\frac{\partial f}{\partial y} \frac{d y}{d t}
</script>
</div>
<p><strong>Backpropagation for RNNs: Proof sketch</strong></p>
<p><img alt="1560960544222" src="../imgs/1560960544222.png" /></p>
<p><strong>Backpropagation for RNNs</strong></p>
<p><img alt="1560960622191" src="../imgs/1560960622191.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ： 如何计算？</li>
<li><strong><u>回答</u></strong> ：反向传播的时间步长 <span><span class="MathJax_Preview">i=t,\dots,0</span><script type="math/tex">i=t,\dots,0</script></span> 。累加梯度。这个算法叫做“<strong>backpropagation through time</strong>”</li>
</ul>
<p><strong>Generating text with a RNN Language Model</strong></p>
<p>就像n-gram语言模型一样，您可以使用RNN语言模型通过 <strong>重复采样</strong> 来 <strong>生成文本</strong> 。采样输出是下一步的输入。</p>
<p><img alt="1560960753273" src="../imgs/1560960753273.png" /></p>
<ul>
<li>相比n-gram更流畅，语法正确，但总体上仍然很不连贯</li>
<li>食谱的例子中，生成的文本并没有记住文本的主题是什么</li>
<li>
<p>哈利波特的例子中，甚至有体现出了人物的特点，并且引号的开闭也没有出现问题</p>
<ul>
<li>也许某些神经元或者隐藏状态在跟踪模型的输出是否在引号中</li>
</ul>
</li>
<li>
<p>RNN是否可以和手工规则结合？</p>
<ul>
<li>例如Beam Serach，但是可能很难做到</li>
</ul>
</li>
</ul>
<p><strong>Evaluating Language Models</strong></p>
<ul>
<li>标准语言模型评估指标是 <strong>perplexity</strong> 困惑度</li>
</ul>
<p><img alt="1560961990924" src="../imgs/1560961990924.png" /></p>
<ul>
<li>这等于交叉熵损失 <span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span> 的指数</li>
</ul>
<div>
<div class="MathJax_Preview">
=\prod_{t=1}^{T}\left(\frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1 / T}=\exp \left(\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\right)=\exp (J(\theta))
</div>
<script type="math/tex; mode=display">
=\prod_{t=1}^{T}\left(\frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1 / T}=\exp \left(\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\right)=\exp (J(\theta))
</script>
</div>
<ul>
<li>低困惑度是更好的</li>
</ul>
<p><strong>RNNs have greatly improved perplexity</strong></p>
<p><img alt="1560962257990" src="../imgs/1560962257990.png" /></p>
<p><strong>Why should we care about Language Modeling?</strong></p>
<ul>
<li>语言模型是一项 <strong>基准测试</strong> 任务，它帮助我们 <strong>衡量</strong> 我们在理解语言方面的 <strong>进展</strong><ul>
<li>生成下一个单词，需要语法，句法，逻辑，推理，现实世界的知识等</li>
</ul>
</li>
<li>语言建模是许多NLP任务的 <strong>子组件</strong>，尤其是那些涉及 <strong>生成文本</strong> 或 <strong>估计文本概率</strong> 的任务<ul>
<li>预测性打字</li>
<li>语音识别</li>
<li>手写识别</li>
<li>拼写/语法纠正</li>
<li>作者识别</li>
<li>机器翻译</li>
<li>摘要</li>
<li>对话</li>
<li>等等</li>
</ul>
</li>
</ul>
<p><strong>Recap</strong></p>
<ul>
<li>语言模型： <strong>预测下一个单词</strong> 的系统</li>
<li>递归神经网络：一系列神经网络<ul>
<li>采用任意长度的顺序输入</li>
<li>在每一步上应用相同的权重</li>
<li>可以选择在每一步上生成输出</li>
</ul>
</li>
<li>递归神经网络<span><span class="MathJax_Preview">\neq</span><script type="math/tex">\neq</script></span>语言模型</li>
<li>我们已经证明，RNNs是构建LM的一个很好的方法。</li>
<li>但RNNs的用处要大得多!</li>
</ul>
<p><strong>RNNs can be used for tagging</strong> </p>
<p>e.g. <u>part-of-speech tagging</u>, named entity recognition</p>
<p><img alt="1560963423563" src="../imgs/1560963423563.png" /></p>
<p><strong>RNNs can be used for sentence classification</strong></p>
<p>e.g. <u>sentiment classification</u></p>
<p><img alt="1560963469195" src="../imgs/1560963469195.png" /></p>
<p><img alt="1560963485192" src="../imgs/1560963485192.png" /></p>
<p>如何计算句子编码</p>
<ul>
<li>使用最终隐层状态</li>
<li>使用所有隐层状态的逐元素最值或均值</li>
</ul>
<p><strong>RNNs can be used as an encoder module</strong> </p>
<p>e.g. <u>question answering</u>, machine translation, many other tasks!</p>
<p><img alt="1560963507012" src="../imgs/1560963507012.png" /></p>
<p>Encoder的结构在NLP中非常常见</p>
<p><strong>RNN-LMs can be used to generate text</strong> </p>
<p>e.g. <u>speech recognition</u>, machine translation, summarization</p>
<p><img alt="1560963533367" src="../imgs/1560963533367.png" /></p>
<p>这是一个条件语言模型的示例。我们使用语言模型组件，并且最关键的是，我们根据条件来调整它</p>
<p>稍后我们会更详细地看到机器翻译。</p>
<p><strong>A note on terminology</strong></p>
<p>本课提到的RNN是 ““vanilla RNN”</p>
<p>下节课将会学习GRU和LSTM以及多层RNN</p>
<p>本课程结束时，你会理解类似“stacked bidirectional LSTM with residual connections and self-attention”的短语</p>
<p><img alt="1560963659418" src="../imgs/1560963659418.png" /></p>
<h2 id="notes-05-language-models-rnn-gru-and-lstm">Notes 05 Language Models, RNN, GRU and LSTM<a class="headerlink" href="#notes-05-language-models-rnn-gru-and-lstm" title="Permanent link">&para;</a></h2>
<details class="abstract"><summary>Keyphrases</summary><p>Language Models. RNN. Bi-directional RNN. Deep RNN. GRU. LSTM.</p>
</details>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://item.jd.com/12355569.html">《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p><h2 id="__comments">评论</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
      this.page.url = "https://looperxx.github.io/CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models/";
      this.page.identifier =
        "CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//https-looperxx-github-io-my-wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();</script></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../CS224n-2019-05-Linguistic Structure Dependency Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">后退</span>05 Linguistic Structure Dependency Parsing</span></div></a><a href="../面经/" title="我的面经" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">前进</span>我的面经</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright"><div class="md-footer-copyright__highlight">Copyright &copy; 2019 - 2020 Looper Xiao Xu</div>powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../assets/fonts/font-awesome.css"> <a href="https://github.com/looperXX" class="md-footer-social__link fa fa-github"></a>  <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" class="md-footer-social__link fa fa-linkedin"></a> </div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="../js/baidu-tongji.js"></script></body></html>