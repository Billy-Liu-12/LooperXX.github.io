<!doctype html><html lang="zh" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="description" content="Looper's homepage"><link rel="canonical" href="https://looperxx.github.io/CS224n-2019-Assignment/"><meta name="author" content="Looper - Xiao Xu"><meta name="lang:clipboard.copy" content="复制"><meta name="lang:clipboard.copied" content="已复制"><meta name="lang:search.language" content="jp"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="没有找到符合条件的结果"><meta name="lang:search.result.one" content="找到 1 个符合条件的结果"><meta name="lang:search.result.other" content="# 个符合条件的结果"><meta name="lang:search.tokenizer" content="[\uff0c\u3002]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>CS224n-2019作业笔记 - Science is interesting.</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#cs224n-2019-assignment" tabindex="1" class="md-skip">跳转至 </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-header-nav__button md-logo"><i class="md-icon"></i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Science is interesting.</span><span class="md-header-nav__topic">CS224n-2019作业笔记</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">键入以开始搜索</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div></div></div></nav></header><div class="md-container"><nav class="md-tabs md-tabs--active" data-md-component="tabs"><div class="md-tabs__inner md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a href=".." title="Home" class="md-tabs__link">Home</a></li><li class="md-tabs__item"><a href="../Linux/" title="Math & CS & Coding" class="md-tabs__link">Math & CS & Coding</a></li><li class="md-tabs__item"><a href="../Attention/" title="ML & DL" class="md-tabs__link">ML & DL</a></li><li class="md-tabs__item"><a href="../自然语言处理简介/" title="NLP" class="md-tabs__link">NLP</a></li><li class="md-tabs__item"><a href="../面经/" title="Interview experience" class="md-tabs__link">Interview experience</a></li><li class="md-tabs__item"><a href="../MkDocs_demo/" title="For MkDocs" class="md-tabs__link">For MkDocs</a></li></ul></div></nav><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-nav__button md-logo"><i class="md-icon"></i></a>Science is interesting.</label><div class="md-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2"><label class="md-nav__link" for="nav-2">Math & CS & Coding</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Math & CS & Coding</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Linux/" title="Linux" class="md-nav__link">Linux</a></li><li class="md-nav__item"><a href="../Coding Knowledge/" title="重点内容" class="md-nav__link">重点内容</a></li><li class="md-nav__item"><a href="../历年机试/" title="历年机试" class="md-nav__link">历年机试</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">ML & DL</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">ML & DL</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Attention/" title="Attention" class="md-nav__link">Attention</a></li><li class="md-nav__item"><a href="../Normalization/" title="Normalization" class="md-nav__link">Normalization</a></li><li class="md-nav__item"><a href="../Concepts/" title="Concepts" class="md-nav__link">Concepts</a></li><li class="md-nav__item"><a href="../花书经验法则/" title="花书经验法则" class="md-nav__link">花书经验法则</a></li><li class="md-nav__item"><a href="../经典网络/" title="经典网络" class="md-nav__link">经典网络</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked><label class="md-nav__link" for="nav-4">NLP</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">NLP</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1"><label class="md-nav__link" for="nav-4-1">简介</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-1">简介</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../自然语言处理简介/" title="自然语言处理简介" class="md-nav__link">自然语言处理简介</a></li><li class="md-nav__item"><a href="../NLP的巨人肩膀/" title="NLP的巨人肩膀" class="md-nav__link">NLP的巨人肩膀</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2"><label class="md-nav__link" for="nav-4-2">书籍笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-2">书籍笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../NLP Concepts/" title="NLP Concepts" class="md-nav__link">NLP Concepts</a></li><li class="md-nav__item"><a href="../Neural Reading Comprehension and beyond/" title="Machine Reading Comprehension" class="md-nav__link">Machine Reading Comprehension</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3" checked><label class="md-nav__link" for="nav-4-3">课程笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-3">课程笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-nav__link">CS224n-2019简介</a></li><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">CS224n-2019作业笔记</label><a href="./" title="CS224n-2019作业笔记" class="md-nav__link md-nav__link--active">CS224n-2019作业笔记</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li><li class="md-nav__item"><a href="#2-coding-implementing-word2vec" title="2 Coding: Implementing word2vec" class="md-nav__link">2 Coding: Implementing word2vec</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#word2vecpy" title="word2vec.py" class="md-nav__link">word2vec.py</a></li><li class="md-nav__item"><a href="#sgdpy" title="sgd.py" class="md-nav__link">sgd.py</a></li><li class="md-nav__item"><a href="#runpy" title="run.py" class="md-nav__link">run.py</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#assignment-03" title="Assignment 03" class="md-nav__link">Assignment 03</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-machine-learning-neural-networks" title="1. Machine Learning &amp; Neural Networks" class="md-nav__link">1. Machine Learning &amp; Neural Networks</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#a-adam-optimizer" title="(a) Adam Optimizer" class="md-nav__link">(a) Adam Optimizer</a></li><li class="md-nav__item"><a href="#b-dropout" title="(b) Dropout" class="md-nav__link">(b) Dropout</a></li></ul></nav></li><li class="md-nav__item"><a href="#2-neural-transition-based-dependency-parsing" title="2. Neural Transition-Based Dependency Parsing" class="md-nav__link">2. Neural Transition-Based Dependency Parsing</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#reference_1" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3"><label class="md-nav__link" for="nav-4-3-3">CS224n-2019学习笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="3"><label class="md-nav__title" for="nav-4-3-3">CS224n-2019学习笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">01 Introduction and Word Vectors</a></li><li class="md-nav__item"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">02 Word Vectors 2 and Word Senses</a></li><li class="md-nav__item"><a href="../CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">03 Word Window Classification,Neural Networks, and Matrix Calculus</a></li><li class="md-nav__item"><a href="../CS224n-2019-04-Backpropagation and Computation Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">04 Backpropagation and Computation Graphs</a></li><li class="md-nav__item"><a href="../CS224n-2019-05-Linguistic Structure Dependency Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">05 Linguistic Structure Dependency Parsing</a></li><li class="md-nav__item"><a href="../CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">06 The probability of a sentence Recurrent Neural Networks and Language Models</a></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5"><label class="md-nav__link" for="nav-5">Interview experience</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-5">Interview experience</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../面经/" title="我的面经" class="md-nav__link">我的面经</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-2" type="checkbox" id="nav-5-2"><label class="md-nav__link" for="nav-5-2">实训笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-5-2">实训笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Protocol Buffers/" title="Protobuf" class="md-nav__link">Protobuf</a></li><li class="md-nav__item"><a href="../FDBus/" title="FDBus" class="md-nav__link">FDBus</a></li><li class="md-nav__item"><a href="../FDBus API/" title="FDBus API" class="md-nav__link">FDBus API</a></li><li class="md-nav__item"><a href="../FDBus内部结构/" title="FDBus内部结构" class="md-nav__link">FDBus内部结构</a></li><li class="md-nav__item"><a href="../Cross compiler/" title="Cross compiler" class="md-nav__link">Cross compiler</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6"><label class="md-nav__link" for="nav-6">For MkDocs</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-6">For MkDocs</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../MkDocs_demo/" title="Demo" class="md-nav__link">Demo</a></li><li class="md-nav__item"><a href="../Material Theme Tutorial/" title="Material Theme Tutorial" class="md-nav__link">Material Theme Tutorial</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li><li class="md-nav__item"><a href="#2-coding-implementing-word2vec" title="2 Coding: Implementing word2vec" class="md-nav__link">2 Coding: Implementing word2vec</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#word2vecpy" title="word2vec.py" class="md-nav__link">word2vec.py</a></li><li class="md-nav__item"><a href="#sgdpy" title="sgd.py" class="md-nav__link">sgd.py</a></li><li class="md-nav__item"><a href="#runpy" title="run.py" class="md-nav__link">run.py</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#assignment-03" title="Assignment 03" class="md-nav__link">Assignment 03</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-machine-learning-neural-networks" title="1. Machine Learning &amp; Neural Networks" class="md-nav__link">1. Machine Learning &amp; Neural Networks</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#a-adam-optimizer" title="(a) Adam Optimizer" class="md-nav__link">(a) Adam Optimizer</a></li><li class="md-nav__item"><a href="#b-dropout" title="(b) Dropout" class="md-nav__link">(b) Dropout</a></li></ul></nav></li><li class="md-nav__item"><a href="#2-neural-transition-based-dependency-parsing" title="2. Neural Transition-Based Dependency Parsing" class="md-nav__link">2. Neural Transition-Based Dependency Parsing</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#reference_1" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/looperxx/looperxx.github.io/edit/master/docs/CS224n-2019-Assignment.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a><h1 id="cs224n-2019-assignment">CS224n-2019 Assignment<a class="headerlink" href="#cs224n-2019-assignment" title="Permanent link">&para;</a></h1>
<p>本文档将记录作业中的要点以及问题的答案</p>
<p>课程笔记参见我的<a href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">博客</a>，并在博客的<a href="https://github.com/LooperXX/LooperXX.github.io">Repo</a>中提供笔记源文件的下载</p>
<h2 id="assignment-01">Assignment 01<a class="headerlink" href="#assignment-01" title="Permanent link">&para;</a></h2>
<ul>
<li>逐步完成共现矩阵的搭建，并调用 <code>sklearn.decomposition</code> 中的 <code>TruncatedSVD</code> 完成传统的基于SVD的降维算法</li>
<li>可视化展示，观察并分析其在二维空间下的聚集情况。</li>
<li>载入Word2Vec，将其与SVD得到的单词分布情况进行对比，分析两者词向量的不同之处。</li>
<li>学习使用<code>gensim</code>，使用<code>Cosine Similarity</code> 分析单词的相似度，对比单词和其同义词与反义词的<code>Cosine Distance</code> ，并尝试找到正确的与错误的类比样例</li>
<li>探寻Word2Vec向量中存在的 <code>Independent Bias</code> 问题</li>
</ul>
<h2 id="assignment-02">Assignment 02<a class="headerlink" href="#assignment-02" title="Permanent link">&para;</a></h2>
<h3 id="1-written-understanding-word2vec">1  Written: Understanding word2vec<a class="headerlink" href="#1-written-understanding-word2vec" title="Permanent link">&para;</a></h3>
<div>
<div class="MathJax_Preview">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</div>
<script type="math/tex; mode=display">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</script>
</div>
<div>
<div class="MathJax_Preview">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</div>
<script type="math/tex; mode=display">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</script>
</div>
<p>真实(离散)概率分布 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 与另一个分布 <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> 的交叉熵损失为 <span><span class="MathJax_Preview">-\sum_i p_{i} \log \left(q_{i}\right)</span><script type="math/tex">-\sum_i p_{i} \log \left(q_{i}\right)</script></span></p>
<div class="admonition question">
<p class="admonition-title">Question a</p>
<p>Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> and <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span>; i.e., show that</p>
<div>
<div class="MathJax_Preview">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</div>
<script type="math/tex; mode=display">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</script>
</div>
<p>Your answer should be one line.</p>
</div>
<p><strong>Answer a</strong> : </p>
<p>因为 <span><span class="MathJax_Preview">\textbf{y}</span><script type="math/tex">\textbf{y}</script></span> 是独热向量，所以 <span><span class="MathJax_Preview">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</span><script type="math/tex">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</script></span> </p>
<div class="admonition question">
<p class="admonition-title">Question b</p>
<p>Compute the partial derivative of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>. Please write your answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer b</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &amp;={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &amp;={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &amp;={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &amp;={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &amp;={U(\hat{y}-y)}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &={U(\hat{y}-y)}
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question c</p>
<p>Compute the partial derivatives of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to each of the ‘outside' word vectors, <span><span class="MathJax_Preview">u_w</span><script type="math/tex">u_w</script></span>'s. There will be two cases: when <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span>, the true ‘outside' word vector, and <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span>, for all other words. Please write you answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer c</strong> : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}<br />
&amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial u_{w}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial u_{w}}} 
\end{array}
$$</p>
<p>When <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= 0 + p(O=w | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= 0 + p(O=w | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c}
\end{array}
</script>
</div>
<p>When <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= -v_c + p(O=o | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c} - v_c
\\ &amp;=(\hat{y}_{w} - 1)v_c
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= -v_c + p(O=o | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c} - v_c
\\ &=(\hat{y}_{w} - 1)v_c
\end{array}
</script>
</div>
<p>Then : </p>
<div>
<div class="MathJax_Preview">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</div>
<script type="math/tex; mode=display">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question d</p>
<p>The sigmoid function is given by the follow Equation :</p>
<div>
<div class="MathJax_Preview">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</div>
<script type="math/tex; mode=display">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</script>
</div>
<p>Please compute the derivative of <span><span class="MathJax_Preview">\sigma (x)</span><script type="math/tex">\sigma (x)</script></span> with respect to <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, where <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a vector.</p>
</div>
<p><strong>Answer d</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&amp;=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&amp;= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &amp;=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} &amp; {0} &amp; {\cdots} &amp; {0} \\ {0} &amp; {\sigma^{\prime}\left(x_{2}\right)} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {0} &amp; {0} &amp; {\cdots} &amp; {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &amp;=\text{diag}(\sigma^\prime(x))
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} & {0} & {\cdots} & {0} \\ {0} & {\sigma^{\prime}\left(x_{2}\right)} & {\cdots} & {0} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {0} & {0} & {\cdots} & {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &=\text{diag}(\sigma^\prime(x))
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question e</p>
<p>Now we shall consider the Negative Sampling loss, which is an alternative to the Naive
Softmax loss. Assume that <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> negative samples (words) are drawn from the vocabulary. For simplicity
of notation we shall refer to them as <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span> and their outside vectors as <span><span class="MathJax_Preview">u_{1}, \dots, u_{K}</span><script type="math/tex">u_{1}, \dots, u_{K}</script></span>. Note that
<span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span>. For a center word <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> and an outside word <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span>, the negative sampling loss function is
given by:</p>
<div>
<div class="MathJax_Preview">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</div>
<script type="math/tex; mode=display">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</script>
</div>
<p>for a sample <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span>, where <span><span class="MathJax_Preview">\sigma(\cdot)</span><script type="math/tex">\sigma(\cdot)</script></span> is the sigmoid function.</p>
<p>Please repeat parts b and c, computing the partial derivatives of <span><span class="MathJax_Preview">J_{\text { neg-sample }}</span><script type="math/tex">J_{\text { neg-sample }}</script></span> respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>, with
respect to <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, and with respect to a negative sample <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>. Please write your answers in terms of the
vectors <span><span class="MathJax_Preview">u_o, v_c,</span><script type="math/tex">u_o, v_c,</script></span> and <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>, where <span><span class="MathJax_Preview">k \in[1, K]</span><script type="math/tex">k \in[1, K]</script></span>. After you've done this, describe with one sentence why this
loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able
to use your solution to part (d) to help compute the necessary gradients here.</p>
</div>
<p><strong>Answer e</strong> : </p>
<p>For <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&amp;=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &amp;= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, Remeber : <span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span> <img alt="😢" class="emojione" src="https://cdn.jsdelivr.net/emojione/assets/4.0/png/64/1f622.png" title=":cry:" /> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</div>
<script type="math/tex; mode=display">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</script>
</div>
<p>Why this
loss function is much more efficient to compute than the naive-softmax loss?</p>
<p>For naive softmax loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&amp;={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&amp;= v_c(\hat y - y)^T
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&= v_c(\hat y - y)^T
\end{array}
</script>
</div>
<p>For negative sampling loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &amp;=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</script>
</div>
<p>从求得的偏导数中我们可以看出，原始的softmax函数每次对 <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> 进行反向传播时，需要与 output vector matrix 进行大量且复杂的矩阵运算，而负采样中的计算复杂度则不再与词表大小 <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> 有关，而是与采样数量 <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 有关。</p>
<div class="admonition question">
<p class="admonition-title">Question f</p>
<p>Suppose the center word is <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and the context window is <span><span class="MathJax_Preview">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</span><script type="math/tex">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</script></span>, where <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> is the context window size. Recall that for the skip-gram version of <strong>word2vec</strong>, the
total loss for the context window is</p>
<div>
<div class="MathJax_Preview">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</div>
<script type="math/tex; mode=display">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</script>
</div>
<p>Here, <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> represents an arbitrary loss term for the center word <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and outside word
<span><span class="MathJax_Preview">w_t+j</span><script type="math/tex">w_t+j</script></span> . <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> could be <span><span class="MathJax_Preview">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> or <span><span class="MathJax_Preview">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span>, depending on your
implementation.</p>
<p>Write down three partial derivatives:</p>
<div>
<div class="MathJax_Preview">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</div>
<script type="math/tex; mode=display">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</script>
</div>
<p>Write your answers in terms of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</script></span> and <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</script></span>. This is very simple -
each solution should be one line.</p>
<p><strong><em>Once you're done</em></strong>: Given that you computed the derivatives of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> with respect to all the
model parameters <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> and <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> in parts a to c, you have now computed the derivatives of the full loss
function <span><span class="MathJax_Preview">J_{skip-gram}</span><script type="math/tex">J_{skip-gram}</script></span> with respect to all parameters. You're ready to implement <strong><em>word2vec</em></strong> !</p>
</div>
<p><strong>Answer f</strong> : 
$$
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\ \frac{\partial J_{s g}}{\partial v_{c}}&amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\ \frac{\partial J_{s g}}{\partial v_{w}}&amp;=0(\text { when } w \neq c) \end{array}
$$</p>
<h3 id="2-coding-implementing-word2vec">2 Coding: Implementing word2vec<a class="headerlink" href="#2-coding-implementing-word2vec" title="Permanent link">&para;</a></h3>
<h4 id="word2vecpy">word2vec.py<a class="headerlink" href="#word2vecpy" title="Permanent link">&para;</a></h4>
<p>本部分要求实现 <span><span class="MathJax_Preview">sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram</span><script type="math/tex">sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram</script></span>  四个函数，主要考察对第一部分中反向传播计算结果的实现。代码实现中，通过优化偏导数结合偏导数计算结果与 <span><span class="MathJax_Preview">\sigma(x) + \sigma(-x) = 1</span><script type="math/tex">\sigma(x) + \sigma(-x) = 1</script></span> 对公式进行转化，从而实现了全矢量化。这部分需要大家自行结合代码与公式进行推导。</p>
<h4 id="sgdpy">sgd.py<a class="headerlink" href="#sgdpy" title="Permanent link">&para;</a></h4>
<p>实现 SGD 
$$
\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)
$$</p>
<h4 id="runpy">run.py<a class="headerlink" href="#runpy" title="Permanent link">&para;</a></h4>
<p>首先要说明的是，这个真的要跑好久 <img alt="😅" class="emojione" src="https://cdn.jsdelivr.net/emojione/assets/4.0/png/64/1f605.png" title=":sweat_smile:" /></p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>Briefly explain in at most three sentences what you see in the plot.</p>
</div>
<p><img alt="word_vectors" src="../imgs/word_vectors.png" /></p>
<p>上图是经过训练的词向量的可视化。我们可以注意到一些模式：</p>
<ul>
<li>近义词被组合在一起，比如 amazing 和 wonderful，woman 和 female。<ul>
<li>但是 man 和 male 却距离较远</li>
</ul>
</li>
<li>反义词可能因为经常属于同一上下文，它们也会与同义词一起出现，比如 enjoyable 和 annoying。</li>
<li><code>man:king::woman:queen</code> 以及 <code>queen:king::female:male</code> 形成的两条直线基本平行</li>
</ul>
<h2 id="assignment-03">Assignment 03<a class="headerlink" href="#assignment-03" title="Permanent link">&para;</a></h2>
<h3 id="1-machine-learning-neural-networks">1. Machine Learning &amp; Neural Networks<a class="headerlink" href="#1-machine-learning-neural-networks" title="Permanent link">&para;</a></h3>
<h4 id="a-adam-optimizer">(a) Adam Optimizer<a class="headerlink" href="#a-adam-optimizer" title="Permanent link">&para;</a></h4>
<p>回忆一下标准随机梯度下降的更新规则
$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\alpha \nabla_{\boldsymbol{\theta}}{J_{\mathrm{minibatch}}(\boldsymbol{\theta})}
$$
其中，<span><span class="MathJax_Preview">\boldsymbol{\theta}</span><script type="math/tex">\boldsymbol{\theta}</script></span> 是包含模型所有参数的向量，<span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> 是损失函数，<span><span class="MathJax_Preview">\nabla_{\boldsymbol{\theta}} J_{\mathrm{minibatch}}(\boldsymbol{\theta})</span><script type="math/tex">\nabla_{\boldsymbol{\theta}} J_{\mathrm{minibatch}}(\boldsymbol{\theta})</script></span> 是关于minibatch数据上参数的损失函数的梯度，<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 是学习率。<a href="https://arxiv.org/pdf/1412.6980.pdf">Adam Optimization</a>使用了一个更复杂的更新规则，并附加了两个步骤。</p>
<div class="admonition question">
<p class="admonition-title">Question 1.a.i</p>
<p>首先，Adam使用了一个叫做 <span><span class="MathJax_Preview">momentum</span><script type="math/tex">momentum</script></span> **动量**的技巧来跟踪梯度的移动平均值 <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\mathbf{m} &amp; \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ \boldsymbol{\theta} &amp; \leftarrow \boldsymbol{\theta}-\alpha \mathbf{m} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ \boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \mathbf{m} \end{aligned}
</script>
</div>
<p>其中，<span><span class="MathJax_Preview">\beta_1</span><script type="math/tex">\beta_1</script></span> 是一个 0 和 1 之间的超参数(通常被设为0.9)。简要说明(不需要用数学方法证明，只需要直观地说明)如何使用m来阻止更新发生大的变化，以及总体上为什么这种小变化可能有助于学习。</p>
</div>
<p><strong>Answer 1.a.i</strong> : </p>
<ul>
<li>
<p>由于超参数 <span><span class="MathJax_Preview">\beta _1</span><script type="math/tex">\beta _1</script></span> 一般被设为0.9，此时对于移动平均的梯度值 <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> 而言，主要受到的是之前梯度的移动平均值的影响，而本次计算得到的梯度将会被缩放为原来的 <span><span class="MathJax_Preview">{1 - \beta_1}</span><script type="math/tex">{1 - \beta_1}</script></span> 倍，即时本次计算得到的梯度很大（梯度爆炸），这一影响也会被减轻，从而阻止更新发生大的变化。</p>
</li>
<li>
<p>通过减小梯度的变化程度，使得每次的梯度更新更加稳定，从而使模型学习更加稳定，收敛速度更快，并且这也减慢了对于较大梯度值的参数的更新速度，保证其更新的稳定性。</p>
</li>
</ul>
<div class="admonition question">
<p class="admonition-title">Question 1.a.ii</p>
<p>Adam还通过跟踪梯度平方的移动平均值 <span><span class="MathJax_Preview">v</span><script type="math/tex">v</script></span> 来使用自适应学习率</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} 
\mathbf{m} &amp; \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ 
\mathbf{v} &amp; \leftarrow \beta_{2} \mathbf{v}+\left(1-\beta_{2}\right)\left(\nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \odot \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta})\right) \\ 
\boldsymbol{\theta} &amp; \leftarrow \boldsymbol{\theta}-\alpha \odot \mathbf{m} / \sqrt{\mathbf{v}}  \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} 
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ 
\mathbf{v} & \leftarrow \beta_{2} \mathbf{v}+\left(1-\beta_{2}\right)\left(\nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \odot \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta})\right) \\ 
\boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \odot \mathbf{m} / \sqrt{\mathbf{v}}  \end{aligned}
</script>
</div>
<p>其中，<span><span class="MathJax_Preview">\odot, /</span><script type="math/tex">\odot, /</script></span> 分别表示逐元素的乘法和除法（所以 <span><span class="MathJax_Preview">z \odot z</span><script type="math/tex">z \odot z</script></span> 是逐元素的平方），<span><span class="MathJax_Preview">\beta_2</span><script type="math/tex">\beta_2</script></span> 是一个 0 和 1 之间的超参数(通常被设为0.99)。因为Adam将更新除以 <span><span class="MathJax_Preview">\sqrt v</span><script type="math/tex">\sqrt v</script></span> ，那么哪个模型参数会得到更大的更新？为什么这对学习有帮助？</p>
</div>
<p><strong>Answer 1.a.ii</strong> : </p>
<ul>
<li>移动平均梯度最小的模型参数将得到较大的更新。</li>
<li>一方面，将梯度较小的参数的更新变大，帮助其走出局部最优点（鞍点）；另一方面，将梯度较大的参数的更新变小，使其更新更加稳定。结合以上两个方面，使学习更加快速的同时也更加稳定。</li>
</ul>
<h4 id="b-dropout">(b) Dropout<a class="headerlink" href="#b-dropout" title="Permanent link">&para;</a></h4>
<p><a href="https://www.cs.toronto.edu/˜hinton/absps/JMLRdropout.pdf">Dropout</a> 是一种正则化技术。在训练期间，Dropout 以 <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> 的概率随机设置隐藏层 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 中的神经元为零(每个minibatch中 dropout 不同的神经元),然后将 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 乘以一个常数 <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 。我们可以写为</p>
<div>
<div class="MathJax_Preview">
\mathbf{h}_{\mathrm{drop}}=\gamma \mathbf{d} \circ \mathbf{h}
</div>
<script type="math/tex; mode=display">
\mathbf{h}_{\mathrm{drop}}=\gamma \mathbf{d} \circ \mathbf{h}
</script>
</div>
<p>其中，<span><span class="MathJax_Preview">d \in \{0,1\}^{D_h}</span><script type="math/tex">d \in \{0,1\}^{D_h}</script></span> ( <span><span class="MathJax_Preview">D_h</span><script type="math/tex">D_h</script></span> 是 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 的大小)是一个掩码向量，其中每个条目都是以 <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> 的概率为 0 ，以 <span><span class="MathJax_Preview">1 - p_{drop}</span><script type="math/tex">1 - p_{drop}</script></span> 的概率为 1。<span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 是使得 <span><span class="MathJax_Preview">h_{drop}</span><script type="math/tex">h_{drop}</script></span> 的期望值为 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 的值</p>
<div>
<div class="MathJax_Preview">
\mathbb{E}_{p_{\text{drop}}}\left[\mathbf{h}_{\text{drop}}\right]_{i}=h_{i}, \text{for all } i \in \{1,\dots,D_h\}
</div>
<script type="math/tex; mode=display">
\mathbb{E}_{p_{\text{drop}}}\left[\mathbf{h}_{\text{drop}}\right]_{i}=h_{i}, \text{for all } i \in \{1,\dots,D_h\}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question 1.b.i</p>
<p><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 必须等于什么(用 <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> 表示) ？简单证明你的答案。</p>
</div>
<p><strong>Answer 1.b.i</strong> : </p>
<div>
<div class="MathJax_Preview">
\gamma = \frac{1}{1 - p_{drop}} \\
</div>
<script type="math/tex; mode=display">
\gamma = \frac{1}{1 - p_{drop}} \\
</script>
</div>
<p>证明如下：</p>
<div>
<div class="MathJax_Preview">
\sum_i (1 -  p_{drop}) h_i = (1 -  p_{drop}) E[h] \\
\sum_i[h_{drop}]_i = \gamma\sum_i (1 -  p_{drop}) h_i = \gamma (1 -  p_{drop}) E[h] = E[h]
</div>
<script type="math/tex; mode=display">
\sum_i (1 -  p_{drop}) h_i = (1 -  p_{drop}) E[h] \\
\sum_i[h_{drop}]_i = \gamma\sum_i (1 -  p_{drop}) h_i = \gamma (1 -  p_{drop}) E[h] = E[h]
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question 1.b.ii</p>
<p>为什么我们应该只在训练时使用 dropout 而在评估时不使用？</p>
</div>
<p><strong>Answer 1.b.ii</strong> : </p>
<p>如果我们在评估期间应用 dropout ，那么评估结果将会具有随机性，并不能体现模型的真实性能，违背了正则化的初衷。通过在评估期间禁用 dropout，从而观察模型的性能与正则化的效果，保证模型的参数得到正确的更新。</p>
<h3 id="2-neural-transition-based-dependency-parsing">2. Neural Transition-Based Dependency Parsing<a class="headerlink" href="#2-neural-transition-based-dependency-parsing" title="Permanent link">&para;</a></h3>
<p>在本节中，您将实现一个基于神经网络的依赖解析器，其目标是在UAS(未标记依存评分)指标上最大化性能。</p>
<p>依存解析器分析句子的语法结构，在 head words 和 修饰 head words 的单词之间建立关系。你的实现将是一个基于转换的解析器，它逐步构建一个解析。每一步都维护一个局部解析，表示如下</p>
<ul>
<li>一个存储正在被处理的单词的 栈 </li>
<li>一个存储尚未处理的单词的 缓存</li>
<li>一个解析器预测的 依赖 的列表</li>
</ul>
<p>最初,栈只包含 ROOT ，依赖项列表是空的，而缓存则包含了这个句子的所有单词。在每一个步骤中,解析器将对部分解析使用一个转换,直到它的魂村是空的，并且栈大小为1。可以使用以下转换：</p>
<ul>
<li>SHIFT：将buffer中的第一个词移出并放到stack上。</li>
<li>LEFT-ARC：将第二个(最近添加的第二)项标记为栈顶元素的依赖，并从堆栈中删除第二项</li>
<li>RIGHT-ARC：将第一个(最近添加的第一)项标记为栈中第二项的依赖，并从堆栈中删除第一项</li>
</ul>
<p>在每个步骤中，解析器将使用一个神经网络分类器在三个转换中决定。</p>
<div class="admonition question">
<p class="admonition-title">Question 2.a</p>
<p>求解解析句子 “I parsed this sentence correctly” 所需的转换顺序。这句话的依赖树如下所示。在每一步中，给出 stack 和 buffer 的结构，以及本步骤应用了什么转换，并添加新的依赖(如果有的话)。下面提供了以下三个步骤。</p>
<p><img alt="1560871900131" src="../imgs/1560871900131.png" /></p>
</div>
<p><strong>Answer 2.a</strong> : </p>
<table>
<thead>
<tr>
<th>Stack</th>
<th>Buffer</th>
<th>New dependency</th>
<th>Transition</th>
</tr>
</thead>
<tbody>
<tr>
<td>[ROOT]</td>
<td>[I, parsed, this, sentence, correctly]</td>
<td></td>
<td>Initial Conﬁguration</td>
</tr>
<tr>
<td>[ROOT, I]</td>
<td>[parsed, this, sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, I, parsed]</td>
<td>[this, sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[this, sentence, correctly]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> I</td>
<td>LEFT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed, this]</td>
<td>[sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed, this, sentence]</td>
<td>[correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed, sentence]</td>
<td>[correctly]</td>
<td>sentence <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> this</td>
<td>LEFT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[correctly]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> sentence</td>
<td>RIGHT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed, correctly]</td>
<td>[]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> correctly</td>
<td>RIGHT-ARC</td>
</tr>
<tr>
<td>[ROOT]</td>
<td>[]</td>
<td>ROOT <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> parsed</td>
<td>RIGHT-ARC</td>
</tr>
</tbody>
</table>
<div class="admonition question">
<p class="admonition-title">Question 2.b</p>
<p>一个包含 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个单词的句子需要多少步(用 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 表示)才能被解析？简要解释为什么。</p>
</div>
<p><strong>Answer 2.b</strong> : </p>
<p>包含<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个单词的句子需要 <span><span class="MathJax_Preview">2 \times n</span><script type="math/tex">2 \times n</script></span> 步才能完成解析。因为需要进行 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 步的 <span><span class="MathJax_Preview">SHIFT</span><script type="math/tex">SHIFT</script></span> 操作和 共计$n 步的 LEFT-ARC 或 RIGHT-ARC 操作，才能完成解析。（每个单词都需要一次SHIFT和ARC的操作，初始化步骤不计算在内）</p>
<p><strong>Question 2.c</strong></p>
<p>实现解析器将使用的转换机制</p>
<p><strong>Question 2.d</strong></p>
<p>我们的网络将预测哪些转换应该应用于部分解析。我们可以使用它来解析一个句子，通过应用预测出的转换操作，直到解析完成。然而，在对大量数据进行预测时，神经网络的运行速度要高得多(即同时预测了对任何不同部分解析的下一个转换)。我们可以用下面的算法来解析小批次的句子</p>
<p><img alt="1560906831993" src="../imgs/1560906831993.png" /></p>
<p>实现minibatch的解析器</p>
<p>我们现在将训练一个神经网络来预测，考虑到栈、缓存和依赖项集合的状态，下一步应该应用哪个转换。首先，模型提取了一个表示当前状态的特征向量。我们将使用原神经依赖解析论文中的特征集合：<a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a>。这个特征向量由标记列表(例如在栈中的最后一个词，缓存中的第一个词，栈中第二到最后一个字的依赖(如果有))组成。它们可以被表示为整数的列表<span><span class="MathJax_Preview">[w_1,w_2,\dots,w_m]</span><script type="math/tex">[w_1,w_2,\dots,w_m]</script></span>，m是特征的数量，每个 <span><span class="MathJax_Preview">0 \leq w_i \lt |V|</span><script type="math/tex">0 \leq w_i \lt |V|</script></span> 是词汇表中的一个token的索引(<span><span class="MathJax_Preview">| V |</span><script type="math/tex">| V |</script></span>是词汇量)。首先，我们的网络查找每个单词的嵌入，并将它们连接成一个输入向量：</p>
<div>
<div class="MathJax_Preview">
\mathbf{x}=\left[\mathbf{E}_{w_{1}}, \dots, \mathbf{E}_{w_{m}}\right] \in \mathbb{R}^{d m}
</div>
<script type="math/tex; mode=display">
\mathbf{x}=\left[\mathbf{E}_{w_{1}}, \dots, \mathbf{E}_{w_{m}}\right] \in \mathbb{R}^{d m}
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\mathbf{E} \in \mathbb{R}^{|V| \times d}</span><script type="math/tex">\mathbf{E} \in \mathbb{R}^{|V| \times d}</script></span> 是嵌入矩阵，每一行 <span><span class="MathJax_Preview">\mathbf{E}_w</span><script type="math/tex">\mathbf{E}_w</script></span> 是一个特定的单词 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的向量。接着我们可以计算我们的预测：</p>
<div>
<div class="MathJax_Preview">
\mathbf h = \text{ReLU}(\mathbf{xW+b_1}) \\
\mathbf l = \text{ReLU}(\mathbf{hU+b_2}) \\
\mathbf {\hat y} = \text{softmax}(l) 
</div>
<script type="math/tex; mode=display">
\mathbf h = \text{ReLU}(\mathbf{xW+b_1}) \\
\mathbf l = \text{ReLU}(\mathbf{hU+b_2}) \\
\mathbf {\hat y} = \text{softmax}(l) 
</script>
</div>
<p>其中， <span><span class="MathJax_Preview">\mathbf{h}</span><script type="math/tex">\mathbf{h}</script></span> 指的是隐藏层，<span><span class="MathJax_Preview">\mathbf{l}</span><script type="math/tex">\mathbf{l}</script></span> 是其分数，<span><span class="MathJax_Preview">\mathbf{\hat y}</span><script type="math/tex">\mathbf{\hat y}</script></span> 指的是预测结果， <span><span class="MathJax_Preview">\text{ReLU(z)}=max(z,0)</span><script type="math/tex">\text{ReLU(z)}=max(z,0)</script></span> 。我们使用最小化交叉熵损失来训练模型</p>
<div>
<div class="MathJax_Preview">
J(\theta) = CE(\mathbf y,\mathbf{\hat y}) = -\sum^3_{i=1}y_i\log\hat y_i
</div>
<script type="math/tex; mode=display">
J(\theta) = CE(\mathbf y,\mathbf{\hat y}) = -\sum^3_{i=1}y_i\log\hat y_i
</script>
</div>
<p>训练集的损失为所有训练样本的 <span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span> 的平均值。</p>
<p>Question 2.e</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<h2 id="reference_1">Reference<a class="headerlink" href="#reference_1" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
</ul><h2 id="__comments">评论</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
      this.page.url = "https://looperxx.github.io/CS224n-2019-Assignment/";
      this.page.identifier =
        "CS224n-2019-Assignment/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//https-looperxx-github-io-my-wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();</script></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">后退</span>CS224n-2019简介</span></div></a><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">前进</span>01 Introduction and Word Vectors</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright"><div class="md-footer-copyright__highlight">Copyright &copy; 2019 - 2020 Looper Xiao Xu</div>powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../assets/fonts/font-awesome.css"> <a href="https://github.com/looperXX" class="md-footer-social__link fa fa-github"></a>  <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" class="md-footer-social__link fa fa-linkedin"></a> </div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="../js/baidu-tongji.js"></script></body></html>