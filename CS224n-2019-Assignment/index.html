<!doctype html><html lang="zh" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="description" content="Looper's homepage"><link rel="canonical" href="https://looperxx.github.io/CS224n-2019-Assignment/"><meta name="author" content="Looper - Xiao Xu"><meta name="lang:clipboard.copy" content="复制"><meta name="lang:clipboard.copied" content="已复制"><meta name="lang:search.language" content="jp"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="没有找到符合条件的结果"><meta name="lang:search.result.one" content="找到 1 个符合条件的结果"><meta name="lang:search.result.other" content="# 个符合条件的结果"><meta name="lang:search.tokenizer" content="[\uff0c\u3002]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>CS224n-2019作业笔记 - Science is interesting.</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#cs224n-2019-assignment" tabindex="1" class="md-skip">跳转至 </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-header-nav__button md-logo"><i class="md-icon"></i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Science is interesting.</span><span class="md-header-nav__topic">CS224n-2019作业笔记</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">键入以开始搜索</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div></div></div></nav></header><div class="md-container"><nav class="md-tabs md-tabs--active" data-md-component="tabs"><div class="md-tabs__inner md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a href=".." title="Home" class="md-tabs__link">Home</a></li><li class="md-tabs__item"><a href="../Linux/" title="Math & CS & Coding" class="md-tabs__link">Math & CS & Coding</a></li><li class="md-tabs__item"><a href="../Attention/" title="ML & DL" class="md-tabs__link">ML & DL</a></li><li class="md-tabs__item"><a href="../自然语言处理简介/" title="NLP" class="md-tabs__link">NLP</a></li><li class="md-tabs__item"><a href="../面经/" title="Interview experience" class="md-tabs__link">Interview experience</a></li><li class="md-tabs__item"><a href="../MkDocs_demo/" title="For MkDocs" class="md-tabs__link">For MkDocs</a></li></ul></div></nav><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-nav__button md-logo"><i class="md-icon"></i></a>Science is interesting.</label><div class="md-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="前往 Github 仓库" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2"><label class="md-nav__link" for="nav-2">Math & CS & Coding</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Math & CS & Coding</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Linux/" title="Linux" class="md-nav__link">Linux</a></li><li class="md-nav__item"><a href="../Coding Knowledge/" title="重点内容" class="md-nav__link">重点内容</a></li><li class="md-nav__item"><a href="../历年机试/" title="历年机试" class="md-nav__link">历年机试</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">ML & DL</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">ML & DL</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Attention/" title="Attention" class="md-nav__link">Attention</a></li><li class="md-nav__item"><a href="../Normalization/" title="Normalization" class="md-nav__link">Normalization</a></li><li class="md-nav__item"><a href="../Concepts/" title="Concepts" class="md-nav__link">Concepts</a></li><li class="md-nav__item"><a href="../花书经验法则/" title="花书经验法则" class="md-nav__link">花书经验法则</a></li><li class="md-nav__item"><a href="../经典网络/" title="经典网络" class="md-nav__link">经典网络</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked><label class="md-nav__link" for="nav-4">NLP</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">NLP</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1"><label class="md-nav__link" for="nav-4-1">简介</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-1">简介</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../自然语言处理简介/" title="自然语言处理简介" class="md-nav__link">自然语言处理简介</a></li><li class="md-nav__item"><a href="../NLP的巨人肩膀/" title="NLP的巨人肩膀" class="md-nav__link">NLP的巨人肩膀</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2"><label class="md-nav__link" for="nav-4-2">书籍笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-2">书籍笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../NLP Concepts/" title="NLP Concepts" class="md-nav__link">NLP Concepts</a></li><li class="md-nav__item"><a href="../Neural Reading Comprehension and beyond/" title="Machine Reading Comprehension" class="md-nav__link">Machine Reading Comprehension</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3" checked><label class="md-nav__link" for="nav-4-3">课程笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-3">课程笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-nav__link">CS224n-2019简介</a></li><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">CS224n-2019作业笔记</label><a href="./" title="CS224n-2019作业笔记" class="md-nav__link md-nav__link--active">CS224n-2019作业笔记</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3"><label class="md-nav__link" for="nav-4-3-3">CS224n-2019学习笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="3"><label class="md-nav__title" for="nav-4-3-3">CS224n-2019学习笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">01 Introduction and Word Vectors</a></li><li class="md-nav__item"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">02 Word Vectors 2 and Word Senses</a></li><li class="md-nav__item"><a href="../CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">03 Word Window Classification,Neural Networks, and Matrix Calculus</a></li><li class="md-nav__item"><a href="../CS224n-2019-04-Backpropagation and Computation Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">04 Backpropagation and Computation Graphs</a></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5"><label class="md-nav__link" for="nav-5">Interview experience</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-5">Interview experience</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../面经/" title="我的面经" class="md-nav__link">我的面经</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-2" type="checkbox" id="nav-5-2"><label class="md-nav__link" for="nav-5-2">实训笔记</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-5-2">实训笔记</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Protocol Buffers/" title="Protobuf" class="md-nav__link">Protobuf</a></li><li class="md-nav__item"><a href="../FDBus/" title="FDBus" class="md-nav__link">FDBus</a></li><li class="md-nav__item"><a href="../FDBus API/" title="FDBus API" class="md-nav__link">FDBus API</a></li><li class="md-nav__item"><a href="../FDBus内部结构/" title="FDBus内部结构" class="md-nav__link">FDBus内部结构</a></li><li class="md-nav__item"><a href="../Cross compiler/" title="Cross compiler" class="md-nav__link">Cross compiler</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6"><label class="md-nav__link" for="nav-6">For MkDocs</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-6">For MkDocs</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../MkDocs_demo/" title="Demo" class="md-nav__link">Demo</a></li><li class="md-nav__item"><a href="../Material Theme Tutorial/" title="Material Theme Tutorial" class="md-nav__link">Material Theme Tutorial</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">目录</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="评论" class="md-nav__link md-nav__link--active">评论</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/looperxx/looperxx.github.io/edit/master/docs/CS224n-2019-Assignment.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a><h1 id="cs224n-2019-assignment">CS224n-2019 Assignment<a class="headerlink" href="#cs224n-2019-assignment" title="Permanent link">&para;</a></h1>
<p>本文档将简要记录作业中的要点</p>
<p>课程笔记参见<a href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">此处</a></p>
<h2 id="assignment-01">Assignment 01<a class="headerlink" href="#assignment-01" title="Permanent link">&para;</a></h2>
<ul>
<li>逐步完成共现矩阵的搭建，并调用 <code>sklearn.decomposition</code> 中的 <code>TruncatedSVD</code> 完成传统的基于SVD的降维算法</li>
<li>可视化展示，观察并分析其在二维空间下的聚集情况。</li>
<li>载入Word2Vec，将其与SVD得到的单词分布情况进行对比，分析两者词向量的不同之处。</li>
<li>学习使用<code>gensim</code>，使用<code>Cosine Similarity</code> 分析单词的相似度，对比单词和其同义词与反义词的<code>Cosine Distance</code> ，并尝试找到正确的与错误的类比样例</li>
<li>探寻Word2Vec向量中存在的 <code>Independent Bias</code> 问题</li>
</ul>
<h2 id="assignment-02">Assignment 02<a class="headerlink" href="#assignment-02" title="Permanent link">&para;</a></h2>
<h3 id="1-written-understanding-word2vec">1  Written: Understanding word2vec<a class="headerlink" href="#1-written-understanding-word2vec" title="Permanent link">&para;</a></h3>
<div>
<div class="MathJax_Preview">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</div>
<script type="math/tex; mode=display">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</script>
</div>
<div>
<div class="MathJax_Preview">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</div>
<script type="math/tex; mode=display">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</script>
</div>
<p>真实(离散)概率分布 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 与另一个分布 <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> 的交叉熵损失为 <span><span class="MathJax_Preview">-\sum_i p_{i} \log \left(q_{i}\right)</span><script type="math/tex">-\sum_i p_{i} \log \left(q_{i}\right)</script></span></p>
<div class="admonition question">
<p class="admonition-title">Question a</p>
<p>Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> and <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span>; i.e., show that</p>
<div>
<div class="MathJax_Preview">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</div>
<script type="math/tex; mode=display">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</script>
</div>
<p>Your answer should be one line.</p>
</div>
<p><strong>Answer a</strong> : </p>
<p>因为 <span><span class="MathJax_Preview">\textbf{y}</span><script type="math/tex">\textbf{y}</script></span> 是独热向量，所以 <span><span class="MathJax_Preview">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</span><script type="math/tex">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</script></span> </p>
<div class="admonition question">
<p class="admonition-title">Question b</p>
<p>Compute the partial derivative of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>. Please write your answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer b</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &amp;={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &amp;={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &amp;={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &amp;={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &amp;={U(\hat{y}-y)}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &={U(\hat{y}-y)}
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question c</p>
<p>Compute the partial derivatives of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to each of the ‘outside' word vectors, <span><span class="MathJax_Preview">u_w</span><script type="math/tex">u_w</script></span>'s. There will be two cases: when <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span>, the true ‘outside' word vector, and <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span>, for all other words. Please write you answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer c</strong> : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}<br />
&amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial u_{w}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial u_{w}}} 
\end{array}
$$</p>
<p>When <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= 0 + p(O=w | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= 0 + p(O=w | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c}
\end{array}
</script>
</div>
<p>When <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= -v_c + p(O=o | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c} - v_c
\\ &amp;=(\hat{y}_{w} - 1)v_c
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= -v_c + p(O=o | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c} - v_c
\\ &=(\hat{y}_{w} - 1)v_c
\end{array}
</script>
</div>
<p>Then : </p>
<div>
<div class="MathJax_Preview">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</div>
<script type="math/tex; mode=display">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question d</p>
<p>The sigmoid function is given by the follow Equation :</p>
<div>
<div class="MathJax_Preview">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</div>
<script type="math/tex; mode=display">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</script>
</div>
<p>Please compute the derivative of <span><span class="MathJax_Preview">\sigma (x)</span><script type="math/tex">\sigma (x)</script></span> with respect to <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, where <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a vector.</p>
</div>
<p><strong>Answer d</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&amp;=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&amp;= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &amp;=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} &amp; {0} &amp; {\cdots} &amp; {0} \\ {0} &amp; {\sigma^{\prime}\left(x_{2}\right)} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {0} &amp; {0} &amp; {\cdots} &amp; {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &amp;=\text{diag}(\sigma^\prime(x))
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} & {0} & {\cdots} & {0} \\ {0} & {\sigma^{\prime}\left(x_{2}\right)} & {\cdots} & {0} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {0} & {0} & {\cdots} & {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &=\text{diag}(\sigma^\prime(x))
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question e</p>
<p>Now we shall consider the Negative Sampling loss, which is an alternative to the Naive
Softmax loss. Assume that <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> negative samples (words) are drawn from the vocabulary. For simplicity
of notation we shall refer to them as <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span> and their outside vectors as <span><span class="MathJax_Preview">u_{1}, \dots, u_{K}</span><script type="math/tex">u_{1}, \dots, u_{K}</script></span>. Note that
<span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span>. For a center word <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> and an outside word <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span>, the negative sampling loss function is
given by:</p>
<div>
<div class="MathJax_Preview">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</div>
<script type="math/tex; mode=display">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</script>
</div>
<p>for a sample <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span>, where <span><span class="MathJax_Preview">\sigma(\cdot)</span><script type="math/tex">\sigma(\cdot)</script></span> is the sigmoid function.</p>
<p>Please repeat parts b and c, computing the partial derivatives of <span><span class="MathJax_Preview">J_{\text { neg-sample }}</span><script type="math/tex">J_{\text { neg-sample }}</script></span> respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>, with
respect to <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, and with respect to a negative sample <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>. Please write your answers in terms of the
vectors <span><span class="MathJax_Preview">u_o, v_c,</span><script type="math/tex">u_o, v_c,</script></span> and <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>, where <span><span class="MathJax_Preview">k \in[1, K]</span><script type="math/tex">k \in[1, K]</script></span>. After you've done this, describe with one sentence why this
loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able
to use your solution to part (d) to help compute the necessary gradients here.</p>
</div>
<p><strong>Answer e</strong> : </p>
<p>For <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&amp;=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &amp;= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\left(\sigma\left(u_{k}^{T} v_{c}\right)\right) u_{k}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\left(\sigma\left(u_{k}^{T} v_{c}\right)\right) u_{k}
\end{array}
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, Remeber : <span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span> <img alt="😢" class="emojione" src="https://cdn.jsdelivr.net/emojione/assets/4.0/png/64/1f622.png" title=":cry:" /> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</div>
<script type="math/tex; mode=display">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</script>
</div>
<p>Why this
loss function is much more efficient to compute than the naive-softmax loss?</p>
<p>For naive softmax loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&amp;={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&amp;= v_c(\hat y - y)^T
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&= v_c(\hat y - y)^T
\end{array}
</script>
</div>
<p>For negative sampling loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\left(\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right) \boldsymbol{u}_{k} \\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} \\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &amp;=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\left(\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right) \boldsymbol{u}_{k} \\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} \\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</script>
</div>
<p>从求得的偏导数中我们可以看出，原始的softmax函数每次对 <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> 进行反向传播时，需要与 output vector matrix 进行大量且复杂的矩阵运算，而负采样中的计算复杂度则不再与词表大小 <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> 有关，而是与采样数量 <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 有关。</p>
<div class="admonition question">
<p class="admonition-title">Question f</p>
<p>Suppose the center word is <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and the context window is <span><span class="MathJax_Preview">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</span><script type="math/tex">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</script></span>, where <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> is the context window size. Recall that for the skip-gram version of <strong>word2vec</strong>, the
total loss for the context window is</p>
<div>
<div class="MathJax_Preview">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</div>
<script type="math/tex; mode=display">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</script>
</div>
<p>Here, <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> represents an arbitrary loss term for the center word <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and outside word
<span><span class="MathJax_Preview">w_t+j</span><script type="math/tex">w_t+j</script></span> . <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> could be <span><span class="MathJax_Preview">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> or <span><span class="MathJax_Preview">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span>, depending on your
implementation.</p>
<p>Write down three partial derivatives:</p>
<div>
<div class="MathJax_Preview">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</div>
<script type="math/tex; mode=display">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</script>
</div>
<p>Write your answers in terms of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</script></span> and <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</script></span>. This is very simple -
each solution should be one line.</p>
<p><strong><em>Once you're done</em></strong>: Given that you computed the derivatives of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> with respect to all the
model parameters <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> and <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> in parts a to c, you have now computed the derivatives of the full loss
function <span><span class="MathJax_Preview">J_{skip-gram}</span><script type="math/tex">J_{skip-gram}</script></span> with respect to all parameters. You're ready to implement <strong><em>word2vec</em></strong> !</p>
</div>
<p><strong>Answer f</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\\ \frac{\partial J_{s g}}{\partial v_{c}}&amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\\ \frac{\partial J_{s g}}{\partial v_{w}}&amp;=0(\text { when } w \neq c) \end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\\ \frac{\partial J_{s g}}{\partial v_{c}}&= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\\ \frac{\partial J_{s g}}{\partial v_{w}}&=0(\text { when } w \neq c) \end{array}
</script>
</div>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://my.oschina.net/findbill/blog/535044">从SVD到PCA——奇妙的数学游戏</a></li>
</ul><h2 id="__comments">评论</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
      this.page.url = "https://looperxx.github.io/CS224n-2019-Assignment/";
      this.page.identifier =
        "CS224n-2019-Assignment/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//https-looperxx-github-io-my-wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();</script></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../CS224n-2019 简介/" title="CS224n-2019简介" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">后退</span>CS224n-2019简介</span></div></a><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">前进</span>01 Introduction and Word Vectors</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright"><div class="md-footer-copyright__highlight">Copyright &copy; 2019 - 2020 Looper Xiao Xu</div>powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../assets/fonts/font-awesome.css"> <a href="https://github.com/looperXX" class="md-footer-social__link fa fa-github"></a>  <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" class="md-footer-social__link fa fa-linkedin"></a> </div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="../js/baidu-tongji.js"></script></body></html>