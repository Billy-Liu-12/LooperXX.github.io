# CS224n-2019 笔记

-   结合每课时的课件、笔记与推荐读物等整理而成

-   作业部分将单独整理

## Lecture 01 Introduction and Word Vectors

**Lecture Plan**

-   The course
-   Human language and word meaning
-   Word2vec introduction
-   Word2vec objective function gradients
-   Optimization basics
-   Looking at word vectors

>   Singular Value Decomposition (SVD) 奇异值分解

### Human language and word meaning

人类之所以比类人猿更“聪明”，是因为我们有语言，因此是一个人机网络，其中人类语言作为网络语言。人类语言具有**信息功能**和**社会功能**。

据估计，人类语言只有大约5000年的短暂历。语言是人类变得强大的主要原因。写作是另一件让人类变得强大的事情。它是使知识能够在空间上传送到世界各地，并在时间上传送的一种工具。

但是，相较于如今的互联网的传播速度而言，人类语言是一种缓慢的语言。然而，只需人类语言形式的几百位信息，就可以构建整个视觉场景。这就是自然语言如此迷人的原因。

**How do we represent the meaning of a word?**

***meaning***

-   用一个词、词组等表示的概念。
-   一个人想用语言、符号等来表达的想法。
-   表达在作品、艺术等方面的思想

理解意义的最普遍的语言方式(***linguistic way***) : 语言符号与语言符号的意义的转化
$$
signifier(symbol)\Leftrightarrow signified(idea \ or \ thing) \\
= \textbf{denotational semantics}
$$

>   denotational semantics 指称语义

**How do we have usable meaning in a computer?**

***WordNet***, 一个包含同义词集和上位词(“is a”关系) ***synonym sets and hypernyms*** 的列表的辞典

```python tab=&quot;synonym&quot;
from nltk.corpus import wordnet as wn
poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}
for synset in wn.synsets("good"):
	print("{}: {}".format(poses[synset.pos()],
					", ".join([l.name() for l in synset.lemmas()])))
```

```python tab=“hypernyms”
from nltk.corpus import wordnet as wn
panda = wn.synset("panda.n.01")
hyper = lambda s: s.hypernyms()
list(panda.closure(hyper))
```

![同义词](imgs/1560068762906.png)

![上位词](imgs/1560068729196.png)

**Problems with resources like WordNet**

-   作为一个资源很好，但忽略了细微差别
    -   例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的。
-   缺少单词的新含义
    -   难以持续更新
    -   例如 wicked, badass, nifty, wizard, genius, ninja, bombest
-   主观的
-   需要人类劳动来创造和调整
-   无法计算单词相似度

**Representing words as discrete symbols**

在传统的自然语言处理中，我们把词语看作离散的符号: hotel, conference, motel - a **localist** representation。单词可以通过独热向量(one-hot vectors，只有一个1，其余均为0的稀疏向量) 。向量维度=词汇量(如500,000)。
$$
motel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0]\\
hotel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0 \  0 \  0 \  0]
$$
**Problem with words as discrete symbols**

所有向量是正交的。对于独热向量，没有关于相似性概念，并且向量维度过大。

**Solutions**

-   使用类似 ***WordNet*** 的工具中的列表，获得相似度，但会因不够完整而失败
-   学习在向量本身中编码相似性

**Representing words by their context**

-   **<u>Distributional semantics</u>** ：一个单词的意思是由经常出现在它附近的单词给出的
    -   *“You shall know a word by the company it keeps”* (J. R. Firth 1957: 11)
    -   现代统计NLP最成功的理念之一
    -   有点物以类聚，人以群分的感觉
-   当一个单词$w$出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)。
-   使用$w$的许多上下文来构建$w$的表示

![示例](imgs/1560069660365.png)

### Word2vec introduction

我们为每个单词构建一个**密集**的向量，使其与出现在相似上下文中的单词向量相似

词向量 ***word vectors*** 有时被称为词嵌入***word embeddings*** 或词表示***word representations*** 

它们是分布式表示 ***distributed representation***
$$
\mathrm{banking} = \left[\begin{matrix}0.286\\0.792\\-0.177\\-0.107\\0.109\\-0.542\\0.349\\0.271\end{matrix}\right]
$$
***Word2vec*** (Mikolov et al. 2013)是一个学习单词向量的**框架**

Idea：

-   我们有大量的文本 (corpus means 'body' in Latin. 复数为corpora)
-   固定词汇表中的每个单词都由一个向量表示
-   文本中的每个位置 $t$，其中有一个中心词 $c$ 和上下文(“外部”)单词 $o$ 
-   使用 $c$ 和 $o$ 的**词向量的相似性**来计算给定 $c$ 的 $o$ 的**概率**(反之亦然)
-   **不断调整词向量**来最大化这个概率

下图为窗口大小 $j$ =2时的 $P\left(w_{t+j} | w_{t}\right)$ 计算过程，center word分别为 $into$ 和 $banking$

![1560070410531](imgs/1560070410531.png)

![1560070494437](imgs/1560070494437.png)

### Word2vec objective function

对于每个位置$t=1, \ldots, T$ ，在大小为$m$的固定窗口内预测上下文单词，给定中心词 $w_j$
$$
Likelihoood = L(\theta) = \prod^{T}_{t=1} \prod_{-m \leq j \leq m \atop j \neq 0} P(w_{t+j} | w_{t} ; \theta)
$$

-   其中，$\theta$ 为所有需要优化的变量

目标函数$J(\theta)$ (有时被称为代价函数或损失函数) 是(平均)负对数似然
$$
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)
$$
其中log形式是方便将联乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。

-   **最小化目标函数 $\Leftrightarrow$  最大化预测精度**
-   <u>问题</u>：如何计算$  P(w_{t+j} | w_{t} ; \theta)$ ？
-   <u>回答</u>：对于每个单词都是用两个向量
    -   $v_w$ 当 $w$ 是中心词时
    -   $u_w$ 当 $w$ 是上下文词时
-   于是对于一个中心词 $c$ 和一个上下文词 $ o$ 

$$
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$

### Word2vec prediction function

$$
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$

-   取幂使任何数都为正
-   点积比较o和c的相似性 $u^{T} v=u . v=\sum_{i=1}^{n} u_{i} v_{i}$ ，点积越大则概率越大
-   分母：对整个词汇表进行标准化，从而给出概率分布

**softmax function** $\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$
$$
\operatorname{softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}
$$
将任意值 $x_i$ 映射到概率分布 $p_i$

-   **max** ：因为放大了最大的概率
-   **soft** ：因为仍然为较小的 $x_i$ 赋予了一定概率
-   深度学习中常用

梯度下降，随机梯度下降以及链式求导法则的知识在此不再赘述，你可以在**Reference**部分找到你需要的资料:smile:

## Notes 01  Introduction, SVD and Word2Vec

**Keyphrases**: Natural Language Processing. Word Vectors. Singular Value Decomposition. Skip-gram. Continuous Bag of Words
(CBOW). Negative Sampling. Hierarchical Softmax. Word2Vec.

这组笔记首先介绍了自然语言处理(NLP)的概念及其面临的问题。然后我们继续讨论将单词表示为数字向量的概念。最后，讨论了常用的词向量设计方法。

### Introduction to Natural Language Processing

**What is so special about NLP?**

Natural language is a discrete/symbolic/categorical system 离散的/符号的/分类的

人类的语言有什么特别之处?人类语言是一个专门用来表达意义的系统，而不是由任何形式的物理表现产生的。在这方面上，它与视觉或任何其他机器学习任务都有很大的不同。

大多数单词只是一个超语言实体的符号：单词是一个映射到所指(想法或事物)的能指。

例如，“rocket”一词指的是火箭的概念，因此可以引申为火箭的实例。当我们使用单词和字母来表达符号时，也会有一些例外，例如“whoompaa”的使用。最重要的是，这些语言的符号可以被 encoded 成几种形式：声音、手势、文字等等，然后通过连续的信号传输给大脑，大脑本身似乎也能以一种连续的方式对这些信号进行 encode。语言哲学和语言学已经做了大量的工作来使人类语言概念化，并区分单词和它们的指代、意义等。不同的任务有不同的难度

**Examples of tasks**



## Reference

以下是学习本课程时的推荐书籍：

[《神经网络与深度学习》](<https://nndl.github.io/>)

以下是整理笔记的过程中参考的博客：

[斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录](<https://zhuanlan.zhihu.com/p/59011576>)

[斯坦福NLP课程 CS224N Winter 2019 学习笔记](<https://zhuanlan.zhihu.com/p/61625439>)

[斯坦福大学 CS224n自然语言处理与深度学习笔记汇总](<https://zhuanlan.zhihu.com/p/31977759>)