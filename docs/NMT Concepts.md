# NMT Concepts

这是一门将一种自然源语言转换为另一种自然目标语言的技术。

近些年来，以数据为驱动的解决方案是研究者们的主流方向，例如统计机器翻译SMT和神经机器翻译NMT。

尽管SMT在过去的二十年中取得了巨大的进步，基于单词、短语以及语法的模型层出不穷，但翻译质量并没有真正达到让用户满意的水准，并且SMT系统变得越来越复杂，许多不同的组件都是单独构建的，这使得进一步开发变得极其困难。SMT的缺点很明显，离散表示的数据稀疏问题、难以解决的长距离依赖问题以及需要专门设计的特征、隐式结构与翻译过程。

而NMT利用连续的表示代替离散表示，利用神经网络实现高维空间下的自然语言之间的相互映射。基于Encoder-Decoder框架和RNN，可以将源语言的语句转化为context vector，再转化为目标语言的语句，尽管可以通过LSTM和GRU等门阀机制帮助缓解梯度消失与梯度爆炸问题，但是其根本缺点在于将任意长度的句子都编码为固定长度。为此，引入注意力机制，例如通过计算encoder和decoder中隐层向量的相关性作为context vector的加权权重，动态计算关联强度以提升长句翻译效果；引入CNN，解决RNN无法并行化计算的问题，并容易捕捉到一些全局的结构信息，再借助残差连接和注意力机制，获得具有更大模型容量、更强模型表示能力的深层模型；使用纯粹的注意力机制Self-Attention，与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的hidden state计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系；但Self Attention不同，它可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系，计算复杂度减小，可并行化并且利于长距离依赖学习。

机器翻译还在持续发展，需要克服领域误匹配、训练数据的总量、生僻词、长句子、词对齐和束搜索的挑战，实现翻译质量、翻译领域、翻译语言种类三个维度的高水平机器翻译。