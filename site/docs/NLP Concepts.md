# NLP Concepts

## NMT Concepts

这是一门将一种自然源语言转换为另一种自然目标语言的技术。

近些年来，以数据为驱动的解决方案是研究者们的主流方向，例如统计机器翻译SMT和神经机器翻译NMT。

尽管SMT在过去的二十年中取得了巨大的进步，基于单词、短语以及语法的模型层出不穷，但翻译质量并没有真正达到让用户满意的水准，并且SMT系统变得越来越复杂，许多不同的组件都是单独构建的，这使得进一步开发变得极其困难。SMT的缺点很明显，离散表示的数据稀疏问题、难以解决的长距离依赖问题以及需要专门设计的特征、隐式结构与翻译过程。

而NMT利用连续的表示代替离散表示，利用神经网络实现高维空间下的自然语言之间的相互映射。基于Encoder-Decoder框架和RNN，可以将源语言的语句转化为context vector，再转化为目标语言的语句，尽管可以通过LSTM和GRU等门阀机制帮助缓解梯度消失与梯度爆炸问题，但是其根本缺点在于将任意长度的句子都编码为固定长度。为此，引入注意力机制，例如通过计算encoder和decoder中隐层向量的相关性作为context vector的加权权重，动态计算关联强度以提升长句翻译效果；引入CNN，解决RNN无法并行化计算的问题，并容易捕捉到一些全局的结构信息，再借助残差连接和注意力机制，获得具有更大模型容量、更强模型表示能力的深层模型；使用纯粹的注意力机制Self-Attention，与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的hidden state计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系；但Self Attention不同，它可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系，计算复杂度减小，可并行化并且利于长距离依赖学习。

机器翻译还在持续发展，需要克服领域误匹配、训练数据的总量、生僻词、长句子、词对齐和束搜索的挑战，实现翻译质量、翻译领域、翻译语言种类三个维度的高水平机器翻译。

## MRC Concepts

关于机器阅读理解，学生认为这是一门让机器像人类一样阅读文本，进而根据对该文本的理解来回答问题的技术。

这一任务主要分为四类：完形填空、单项选择、范围预测以及开放式问答，基于EM、F1 Score和BLEU等指标评价模型。为了降低难度，模型大都将世界知识排除在外。相较于QA，阅读理解则更强调文本理解，并以回答问题的形式衡量语言理解的程度，所以回答理解问题的信息都是来自于文章本身而不是世界知识。

传统的MRC基于(passage,question,answer)的三元组训练出实现f:(passage,question)->answer的统计学模型，高度依赖已有的语言工具，手工设计大量特征，但特征通常非常稀疏并且概括性差。15年大规模数据集和神经模型的出现促进了MRC领域的发展，神经阅读理解具备以下优点：自动学习特征；词嵌入缓解变量稀疏的问题；概念简单，将任务转换为构建神经网络结构的问题。其工作流程主要分为：理解文章，理解问题以及理解文章和问题之间的联系。以THE STANFORD ATTENTIVE READER为例，通过双向LSTM(多层)的Encoder-Decoder框架，首先以词嵌入为问题构建一个向量表示，并且为文中的每一个token构建向量表示(同时考虑单词自身属性以及单词和问题的相关性)，然后计算上下文中问题及其单词之间的相似度函数，对于范围预测问题而言，使用问题-单词相似度评分来决定答案跨度的起始和结束位置，模型对文章/问题编码的所有参数和相似度函数进行联合优化，实现最终的答案预测。随着研究的深入，字符级别的embedding可以提升少见词和词库外单词的表征，依靠上下文动态调整embedding可以解决多义词问题，不同层次的attention组合、双向attention以及使用self-attention机制的Transformer架构的引入，都一定程度上提升了模型的性能。

MRC领域未来需要建立更具备难度，更接近真实应用场景的大规模数据集，并引入世界知识，发展更完善的推理机制，同时需要应用大规模预训练模型，再将阅读理解技术推广到多语言、以及QA等其他NLP任务中。真正做到，更多数据，更多知识，更深理解。

## QA Concepts

这是一门准确地理解以自然语言形式描述的用户提问，并通过检索异构语料库或问答知识库返回简洁、精确的匹配答案的技术。

问答系统之间的区别，主要在问题、数据和答案三个维度上：限定域/开放域的问题、结构化/半结构化/非结构化的数据、抽取式/生成式的答案。

基于信息检索的问答系统是结合关键词匹配和信息抽取的经典浅层语义分析方法，处理流程可以简单地分为问题解析（处理问题并生成问题关键词）、信息检索、答案抽取三步。简而言之，是在 IR 检索出与 Query 相关的文档 Document 的基础上，模型去阅读并理解 Q 和 D，得到答案 Answer。

随后出现了基于问答知识库KB、基于知识图谱KG等方法，问答系统往往是多种机制的组合。传统的基于符号表示的知识库问答是基于知识库的浅层推断，通过实体链接和关系抽取建立自然语言和知识库之间的映射，将问题的语义表示与知识库进行关联，依靠过多的人工介入、规则与模板等，效果始终不如人意。

随着深度学习的发展，神经答案抽取帮助基于IR的QA取得了令人瞩目的效果：THE STANFORD ATTENTIVE READER通过双向LSTM(多层)的Encoder-Decoder框架，首先为问题构建一个向量表示，并且为文中的每一个token构建向量表示(同时考虑单词自身属性以及单词和问题的相关性)，然后计算上下文中问题及其单词之间的相似度函数，对于范围预测问题而言，使用问题-单词相似度评分来决定答案跨度的起始和结束位置，模型对文章/问题编码的所有参数和相似度函数进行联合优化，实现最终的答案预测。

而对于KBQA而言，模型需要对给定的自然语言问题进行语义理解和解析，进而用知识库进行语义匹配、查询与推理得出答案，主要包括知识库的建立以及如何使用该知识库查询/推理答案两个关键部分。通过RDF或图数据库存储结构化/半结构化/非结构化数据，经过信息抽取(实体抽取、关系抽取、属性抽取)，知识融合(实体链接、知识合并)，知识加工(本体构建、知识推理)，知识更新等步骤，建立起完备有效的知识库/知识图谱，完成了将知识、文档、句子、词汇映射到统一语义空间下，实现跨领域、跨对象的知识迁移，提供多任务学习的统一底层表示。再通过语义解析、信息抽取、向量建模等方法，实现问题语义和知识语义的理解和匹配，即完成问题的理解与表示，再将其关联到知识库的结构化查询中完成语义关联。

QA领域目前还在持续发展，对于复杂问题和推理性问题的研究还在继续。